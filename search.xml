<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>PyTorch环境安装</title>
    <url>/2022/01/14/Anaconda%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<h2 id="1-安装Anaconda"><a href="#1-安装Anaconda" class="headerlink" title="1. 安装Anaconda"></a>1. 安装Anaconda</h2><p>包含了众多流行的科学计算、数据分析的 Python 包</p>
<p>下载链接：<a href="https://mirror.tuna.tsinghua.edu.cn/help/anaconda/">anaconda | 镜像站使用帮助 | 清华大学开源软件镜像站 | Tsinghua Open Source Mirror</a></p>
<ul>
<li>Windows环境配置<ul>
<li>版本：Anaconda3-5.2.0</li>
<li>路径：D:\software\Anaconda3</li>
<li>注意事项：跳过安装VScode；取消勾选</li>
<li>判断是否安装成功：开始➡Anaconda Prompt➡出现base安装成功</li>
</ul>
</li>
</ul>
<h2 id="2-显卡"><a href="#2-显卡" class="headerlink" title="2.显卡"></a>2.显卡</h2><p>tensorflow和pytorch都只支持NVIDIA的显卡，对训练起到加速的作用，对学习pytorch没有影响</p>
<p>查看显卡型号和支持的cuda版本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure>
<h2 id="3-管理环境"><a href="#3-管理环境" class="headerlink" title="3. 管理环境"></a>3. 管理环境</h2><ol>
<li><p>创建新环境</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda create -n pytorch python=3.6</span><br></pre></td></tr></table></figure>
<p>pytorch:环境名</p>
<p>包名+版本号</p>
</li>
<li><p>切换环境</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda activate pytorch</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="4-安装PyTorch"><a href="#4-安装PyTorch" class="headerlink" title="4. 安装PyTorch"></a>4. 安装PyTorch</h2><p>下载链接：<a href="https://pytorch.org/">PyTorch</a></p>
<p><img src="https://raw.githubusercontent.com/lyb92nlf/pictures/master/img/202201141412964.png" alt="Custom PyTorch configuration"></p>
<p>检查是否安装成功</p>
<ol>
<li>pip list</li>
<li>进入python，输入<code>import torch</code>（无报错安装成功）和<code>torch.cuda.is_available()</code>(显示true可以使用GPU)</li>
</ol>
<h2 id="5-python编辑器"><a href="#5-python编辑器" class="headerlink" title="5. python编辑器"></a>5. python编辑器</h2><ol>
<li><p>PyCharm</p>
<p><img src="https://raw.githubusercontent.com/lyb92nlf/pictures/master/img/202201141614540.png" alt="PyCharm1"></p>
<p><img src="https://raw.githubusercontent.com/lyb92nlf/pictures/master/img/202201141614685.png" alt="PyCharm2"></p>
</li>
<li><p>Jupyter</p>
<p>在pytorch环境中安装Jupyter</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda install nb_conda</span><br></pre></td></tr></table></figure>
</li>
</ol>
]]></content>
      <categories>
        <category>Python</category>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
        <tag>Anaconda</tag>
        <tag>Jupyter</tag>
      </tags>
  </entry>
  <entry>
    <title>绪论</title>
    <url>/2021/11/13/Chapter01%E7%BB%AA%E8%AE%BA/</url>
    <content><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>机器学习(machine learning)的定义：研究如何通过计算的手段，利用经验来改善系统自身的性能。</p>
<p>在计算机系统中，“经验”通常以“数据”的形式存在。</p>
<p>ML研究的主要内容：在计算机上从数据中产生“模型”的算法。</p>
<p>而模型的作用是，当面对新的数据时，模型会给我们提供一定的判断，即是数据预测。</p>
<script type="math/tex; mode=display">
数据\stackrel{学习算法}{\Longrightarrow}模型\stackrel{新情况}{\Longrightarrow}预测</script><h2 id="基本术语"><a href="#基本术语" class="headerlink" title="基本术语"></a>基本术语</h2><p>数据集<code>data set</code>：数据的集合</p>
<p>示例<code>instance</code>/样本<code>sample</code>：每条数据描述了一个对象的信息，每条数据称之为样本</p>
<p>属性<code>attribute</code>/特征<code>feature</code>：数据描述的是样本在某些方面的性质，称之为属性</p>
<p>属性值<code>attribute value</code>：属性的取值</p>
<p>属性空间<code>attribute space</code>/样本空间<code>sample space</code>/输入空间<code>input space</code>：对于一个样本而言，假如它有n种属性，则组成了一个n维空间，称之为样本空间</p>
<p>维数<code>dimensionality</code>：属性的个数</p>
<p>特征向量<code>feature vector</code>：样本在属性空间中对应的向量</p>
<hr>
<p>学习<code>learning</code>/训练<code>training</code>：从数据集中学得模型的过程</p>
<p>训练数据<code>training data</code>：学习过程中使用的数据</p>
<p>训练样本<code>training sample</code>：训练数据中的样本</p>
<p>训练集<code>training set</code>：训练样本的集合</p>
<p>假设<code>hypothesis</code>：学得的模型对应了数据集中某种潜在的规律</p>
<p>真相/真实<code>ground-truth</code>：数据集本身的潜在的规律。学习的过程就是逼近真相的过程</p>
<p>学习器<code>learner</code>：模型的别称</p>
<hr>
<p>标记<code>label</code>：有关示例结果的信息，一般用y表示</p>
<p>样例<code>example</code>：具有标记信息的示例</p>
<p>标记空间<code>label space</code>/输出空间：所有标记的集合构成的空间</p>
<hr>
<p>分类<code>classification</code>：欲预测的值为离散值</p>
<p>回归<code>regression</code>：欲预测的值为连续值</p>
<p>二分类<code>binary classification</code>：分类的类别只有两类（正类和反类）</p>
<p>多分类<code>multi-class classification</code>：分类的类别＞2</p>
<hr>
<p>测试<code>testing</code>：学得模型后，对其进行预测的过程。机器学习是一个反复的过程，需要重复多次学习、测试、调整，才能得到准确率最高的模型</p>
<p>测试样本<code>testing sample</code>：被预测的样本</p>
<hr>
<p>聚类<code>clustering</code>：无监督学习的一种，将训练集的数据分为若干组，而这些组事先是不知道的</p>
<p>簇<code>cluster</code>：聚类得到的数据分类（组）</p>
<hr>
<p>监督学习<code>supervised learning</code>：训练数据拥有标记信息</p>
<p>无监督学习<code>unsupervised learning</code>：训练数据没有标记信息</p>
<hr>
<p>泛化<code>generalization</code>能力：学得模型适用于新样本的能力。或者说，模型预测数据的精准度</p>
<p>独立同分布<code>independent and identically distributed</code>：简称i.i.d。假设样本空间的全体样本服从一个未知的分布，而我们在学习时使用的样本都是独立的从这个分布上采样获得的</p>
<h2 id="假设空间"><a href="#假设空间" class="headerlink" title="假设空间"></a>假设空间</h2><p>所有假设组成的空间</p>
<p><img src="https://raw.githubusercontent.com/lyb92nlf/pictures/master/img/202111131735707.png" alt="西瓜问题的假设空间"></p>
<p>学习的目的是<strong>泛化</strong>，即通过训练，得到一个模型，而这个模型可以对新样例的标签进行精准的预测。</p>
<p>学习的过程看作是在假设空间中进行搜索的过程，搜索目标是找到与训练集匹配的假设，即能够将训练集中的瓜判断正确的假设</p>
<p>需要注意的是，现实问题中我们常面临很大的假设空间，但学习过程是基于有限样本的训练集进行的，因此，可能有多个假设与训练集一致，即存在着一个与训练集一致的“假设空间”，称为样本空间</p>
<h2 id="归纳偏好"><a href="#归纳偏好" class="headerlink" title="归纳偏好"></a>归纳偏好</h2><p>很多情况下，通过现有的有限的数据集，可以得到多个假设；但是我们必须得到一个最好的模型。这时候，就要从这若干个假设空间中，选择其中的一个，从这个空间中提取ML的模型。</p>
<p>我们可以使用另一个法宝：归纳偏好。机器学习算法在学习的过程中，对某种类型的假设的偏好，称之为归纳偏好。可以简单的理解为，对于上述不同的假设空间，在选择最优模型时，其权重不同。</p>
<p>对于归纳偏好，我们使用奥卡姆剃刀来作为一般的原则，用于引导算法确立“正确”的偏好。奥卡姆梯度是自然科学中最常见的法则之一：若有多个假设与观察一致，则选最简单的那个。</p>
]]></content>
      <categories>
        <category>笔记</category>
        <category>西瓜书</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>西瓜书</tag>
      </tags>
  </entry>
  <entry>
    <title>线性模型</title>
    <url>/2021/11/14/Chapter03%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h1 id="基本形式"><a href="#基本形式" class="headerlink" title="基本形式"></a>基本形式</h1><p>$x=(x_1,x_2,\ldots,x_d)$</p>
<p>线性模型（linear model）试图学得一个通过属性的线性组合来进行预测的函数，即</p>
<script type="math/tex; mode=display">
f(x)=w_1x_1+w_2x_2+\ldots+w_dx_d+b</script><p>一般用向量形式写成</p>
<script type="math/tex; mode=display">
f(x)=w^Tx+b</script><p>w 和 b 学得之后， 模型就得以确定</p>
<h1 id="回归任务"><a href="#回归任务" class="headerlink" title="回归任务"></a>回归任务</h1><div class="tabs" id="回归任务"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#回归任务-1">线性回归（linear regression）</button></li><li class="tab"><button type="button" data-href="#回归任务-2">对数线性回归(log-linear regression)</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="回归任务-1"><p>线性回归的任务是寻找线性模型，使得预测值尽可能接近样本的标记</p>
<p>首先，以<em>单属性</em>为例，给定数据集D</p>
<script type="math/tex; mode=display">
f(x_i)=wx_i+b\;\;\;\;\;D=\lbrace(x_i,y_i)\rbrace_{i=1}^m</script><p>求解(w,b)，列方程，使均方误差最小</p>
<script type="math/tex; mode=display">
\begin{align}
(w^*,b^*)&=\underset{(w,b)}{arg\min}\sum_{i=1}^m(f(x_i)-y_i)^2\\
&= \underset{(w,b)}{arg\min}\sum_{i=1}^m(wx_i+b-y_i)^2
\end{align}</script><p>基于均方误差最小化进行模型求解的方法称为<strong>最小二乘法</strong>，线性回归中，最小二乘法就是找到一条直线，使得所有样本到直线的<strong>欧式距离|f(x)-y|</strong>之和最小</p>
<p>然后，通过使偏导等于0，可以得到w和b</p>
<p>对于<em>多属性</em>，同理列均方误差方程，求偏导，令偏导为0</p>
<script type="math/tex; mode=display">
X=\left[
\begin{matrix}
 x_{11}      & x_{12}      & \cdots & x_{1d}  & 1    \\
 x_{21}      & x_{22}      & \cdots & x_{2d}  & 1    \\
 \vdots & \vdots & \ddots & \vdots \\
 x_{m1}      & x_{m2}      & \cdots & x_{md}  & 1    \\
\end{matrix}
\right]
=\left[
\begin{matrix}
 x_1^T      & 1    \\
 x_2^T      & 1    \\
 \vdots & \vdots   \\
 x_m^T      & 1    \\
\end{matrix}
\right]</script><script type="math/tex; mode=display">
X\left[
\begin{matrix}
 w_1\\
 w_2\\
 \vdots\\
 w_d\\
 b\\
\end{matrix}
\right]=\left[
\begin{matrix}
 f(x_1)\\
 f(x_2)\\
 \vdots \\
 f(x_m)\\
\end{matrix}
\right]\Longrightarrow
XW = Y</script><p>最终$ W = (X^TX)^{-1}X^Ty$</p>
<script type="math/tex; mode=display">
f(x_i)=x_i^TW=x_i^T(X^TX)^{-1}X^Ty</script><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="回归任务-2"><script type="math/tex; mode=display">
\ln y=w^Tx+b\tag{lny与x呈线性关系}</script><p><img src="https://raw.githubusercontent.com/lyb92nlf/pictures/master/img/202111151739916.jpg" alt="对数线性回归" style="zoom:50%;" /></p>
<p>更一般的，考虑存在反函数g，令</p>
<script type="math/tex; mode=display">
g(y)=w^Tx+b</script><p>这样得到的模型称为<strong>广义线性模型</strong><code>generalized linear model</code>，函数 g 称为<strong>联系函数</strong><code>link function</code></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>
<h1 id="分类任务"><a href="#分类任务" class="headerlink" title="分类任务"></a>分类任务</h1><div class="tabs" id="分类任务"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分类任务-1">对数几率回归(logistic regression)</button></li><li class="tab"><button type="button" data-href="#分类任务-2">线性判别分析(Linear Discriminant Analysis)</button></li><li class="tab"><button type="button" data-href="#分类任务-3">多分类问题</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分类任务-1"><p>由广义线性模型，定义对数几率<sup><a href="#fn_1" id="reffn_1">1</a></sup>函数</p>
<script type="math/tex; mode=display">
y=\frac{1}{1+e^{-(w^Tx+b)}}\;\;\;\ln\frac{y}{1-y}=w^Tx+b</script><p>使用线性模型的预测结果去逼近真实标记的对数几率</p>
<p>对数几率回归的<strong>优点</strong>：</p>
<ol>
<li>它直接对分类可能性进行建模，无需事先假设数据分布，避免了假设分布不准确所带来的问题。</li>
<li>它不仅预测出类别，还得到近似概率预测，对许多需利用概率辅助决策的任务很有用。</li>
<li>对率函数是任意阶可导的凸函数，有很好的数学性质，现有的许多数值优化算法都可直接用于求取最优解。</li>
</ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分类任务-2"><p>线性判别分析的<strong>思想</strong>：给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影尽可能接近，异类样例的投影尽可能远离。在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定新样本的类别。</p>
<p><img src="https://raw.githubusercontent.com/lyb92nlf/pictures/master/img/202111151802354.png" alt="LDA"></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分类任务-3"><p>多分类问题的<strong>基本思路</strong>是拆解法，即将多分类的任务拆为若干个二分类任务求解。</p>
<p>给定数据集</p>
<script type="math/tex; mode=display">
D=\lbrace(x_i,y_i)\rbrace_{i=1}^m,y_i\in\lbrace C_1,C_2,\dots,C_N\rbrace\tag{C属于不同类别}</script><div class="tabs" id="拆分策略"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#拆分策略-1">一对一(OvO)</button></li><li class="tab"><button type="button" data-href="#拆分策略-2">一对其余(OvR)</button></li><li class="tab"><button type="button" data-href="#拆分策略-3">多对多(MvM)</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="拆分策略-1"><p><strong>OvO</strong>将这 N 个类别两两配对，从而产生$\frac{N(N-1)}{2}$个二分类任务。</p>
<p>最终结果：被预测的最多的类别</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="拆分策略-2"><p><strong>OvR</strong>是每次将一个类的样例作为正例、所有其他类的样例作为反例来训练 N 个分类器。</p>
<p>最终结果：</p>
<ul>
<li>只有一个分类器预测为正类，则对应的类别标记为最终结果</li>
<li>有多个分类器预测为正类，则取置信度最大的类别作为最终结果</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lyb92nlf/pictures/master/img/202111170940591.jpg" alt="OvO与OvR" style="zoom:50%;" /></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="拆分策略-3"><p><strong>MvM</strong>将多个类别作为正例，多个类别作为反例</p>
<p>而正反例的选取必须由特殊的设计，常用的MvM技术有<strong>纠错输出码ECOC</strong></p>
<p>ECOC的工作步骤：</p>
<ul>
<li><strong>编码：</strong>将N个类别做M次划分，形成M个分类器</li>
<li><strong>解码：</strong>M个分类器对测试样本进行预测得到编码，与各个类别的编码进行比较，返回距离最小的类别作为<em>最终结果</em></li>
</ul>
<p>编码矩阵：</p>
<ul>
<li><strong>二元码：</strong>正类、反类</li>
<li><strong>三元码：</strong>正类、反类、停用类</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lyb92nlf/pictures/master/img/202111170958614.jpg" alt="MvM" style="zoom:50%;" /></p>
<p>其中，海明距离：不相同的个数(停用类算0.5)；欧式距离：差值平方和的1/2次方(停用类算0)</p>
<div class="note success no-icon flat"><p>为什么叫做纠错输出码呢？</p>
<p>假设测试实例在预测时某位出错，不会影响最终的结果</p>
<p>一般来说，同一个学习任务，编码越长，纠错能力越强</p>
<p>同等长度的编码，任意两个位置的编码距离越远，纠错能力越强</p>
</div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>
<h2 id="类别不平衡问题"><a href="#类别不平衡问题" class="headerlink" title="类别不平衡问题"></a>类别不平衡问题</h2><p>分类任务中不同类别的训练样例差别很大的情况</p>
<p>解决方法：</p>
<ol>
<li><p>使正反例的数目接近</p>
<ul>
<li>欠采样 - 减少反例</li>
<li>过采样 - 增加正例</li>
</ul>
</li>
<li><p>再放缩</p>
<p>几率$\frac{y}{1-y}$，y为正例的可能性，假设阈值为0.5</p>
<p>则分类器的决策规则为：</p>
<script type="math/tex; mode=display">
若\frac{y}{1-y}>1,预测为正例</script><p>考虑到正反例数目的差别，令$m^+$为正例的数目，$m^-$为反例的数目，则</p>
<script type="math/tex; mode=display">
若\frac{y}{1-y}>\frac{m^+}{m^-},预测为正例</script><p>正例数目与反例数目的比称为<strong>观测几率</strong></p>
</li>
</ol>
<hr>
<blockquote id="fn_1">
<sup>1</sup>. 样本x为正例的可能性/为反例的可能性<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a>
</blockquote>
]]></content>
      <categories>
        <category>笔记</category>
        <category>西瓜书</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>西瓜书</tag>
      </tags>
  </entry>
  <entry>
    <title>模型评估与选择</title>
    <url>/2021/11/14/Chapter02%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E5%92%8C%E8%AF%84%E4%BC%B0/</url>
    <content><![CDATA[<h1 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h1><ul>
<li><p>错误率<code>error rate</code>：错误样本数占测试样本数的比例</p>
</li>
<li><p>精度<code>accuracy</code>：1 - 错误率</p>
</li>
<li><p>误差<code>error</code>：学习器的实际预测输出与样本的真实输出之间的差异</p>
</li>
<li><p>训练误差<code>training error</code>/经验误差<code>empirical error</code>：学习器在训练集上的误差</p>
</li>
<li><p>泛化误差<code>generalization error</code>：在新样本上的误差</p>
<p>想得到的就是泛化误差较小的模型。在实际希望中，需要的是在新样本上能表现很好的学习器，就需要从训练样本中尽可能学出适用于所有潜在样本的“普遍规律”，用以判别新样本，但如果学习器将训练样本中一些自身特点当作所有潜在样本都会具有的性质，就会导致泛化能力下降。这种现象称为“过拟合”，与其相对的就是“欠拟合”：训练一般性质尚未完成。</p>
</li>
<li><p>过拟合<code>overfitting</code>：形象的讲就是学的太细致，假设训练集样本中叶子含有锯齿，那么得到的学习器会认为只有锯齿的才是叶子。把一些特点当做所有数据共有的特性。</p>
</li>
<li><p>欠拟合<code>underfitting</code>：就是训练样本的一般性质没有学好，书中例子就是训练树叶欠拟合就会误以为绿色都是树叶。</p>
</li>
</ul>
<div class="note info simple"><ol>
<li>相对于过拟合，欠拟合相对容易克服</li>
<li>过拟合只能缓解不能避免</li>
</ol>
</div>
<div class="note primary simple"><p>通常评估模型的好坏由泛化误差决定，但由于泛化误差不能直接获得（事先不知道新样本）</p>
</div>
<h1 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h1><p>首先，区分一下测试集和验证集。测试集是指学得模型在<strong>实际使用中</strong>遇到的样本的集合；验证集是在<strong>学习过程中</strong>用于评估测试的样本集合。</p>
<p>评估方法的基本思想：把数据集D分为训练集S和验证集T，将验证集的测试误差作为泛化误差的近似</p>
<div class="tabs" id="评估方法"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#评估方法-1">留出法（hold-out）</button></li><li class="tab"><button type="button" data-href="#评估方法-2">交叉验证法（cross validation）</button></li><li class="tab"><button type="button" data-href="#评估方法-3">自助法（bootstrapping）</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="评估方法-1"><ol>
<li><p>定义</p>
<p>文字描述：把数据集D分为训练集S和验证集T</p>
<p>数学描述：$ D=S\bigcup T,S\bigcap T=\emptyset $</p>
<blockquote>
<p>假定D包含1000个样本，将其划分为S包含700个样本，T包含300个样本，用S进行训练以后，如果模型在T上有90个样本分类错误，那么其错误率为（90/300）*100%=30%，精度为1-30%=70%。</p>
</blockquote>
</li>
<li><p>要求</p>
<ul>
<li><p><strong>S和T的比例</strong></p>
<p>常取2：1或4：1，一般来说，验证集样本不少于30</p>
</li>
<li><p>S与T的划分要尽可能保持数据分布的一致性</p>
<p>分层采样（<strong>指定训练集和验证集中正反例的比例</strong>）：保留类别的比例</p>
<p>假设数据集包含1000个样本，其中500个正例，500个反例，将其划分为包含70%样本的训练集和30%样本的验证集，按照分层采样的策略，S应该包含350个正例，350个反例。T应该包含150个正例，150个反例。</p>
</li>
</ul>
</li>
<li><p>如何选择正反例</p>
<ul>
<li>对样本排序后按顺序挑选</li>
<li>随机挑选</li>
</ul>
</li>
<li><p>一般策略：多次使用留出法，计算测试误差的平均值</p>
<p>采用分层采样策略，若干次随机挑选样本，重复评估后取平均值</p>
</li>
</ol>
<div class="note success simple"><p>保证S、T和集合内正反例的比例，随机挑选样本，取多次评估的测试误差的平均值</p>
</div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="评估方法-2"><p><img src="https://raw.githubusercontent.com/lyb92nlf/pictures/master/img/202111141112123.png" alt=""></p>
<p>将数据集分为k个大小相似的互斥子集（分层采样、随机划分）。每次用K-1个子集的并集作为训练集，余下的子集作为验证集；获得K组训练/验证集，从而可进行k次训练和测试，最终返回的是这k个测试误差的均值。</p>
<ul>
<li><p>留一法<code>leave-one-out</code>：训练集只有一个样本</p>
<p>虽然不受随机划分方式的影响，但在数据集较大时，训练的花销让人难以忍受</p>
</li>
</ul><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="评估方法-3"><p>我们希望评估的是用整个D训练出的模型。但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比D小，这必然会引入一些因训练样本规模不同而导致的估计偏差。</p>
<p>给定包含m个样本的数据集D，每次随机从D 中挑选一个样本，将其拷贝放入D’，然后再将该样本放回初始数据集D中，使得该样本在下次采样时仍有可能被采到。重复执行m 次，就可以得到包含m个样本的数据集D’。于是可以将D’作为训练集，D\D’作为验证集。通过自助采样，初始样本集D中大约有36.8%的样本没有出现在D’中。</p>
<p>自助法在数据集较小，难以有效划分训练集/测试集时很有用，但由于自助法产生的数据集（随机抽样）改变了初始数据集的分布，因此引入了估计偏差。在初始数据集足够时，留出法和交叉验证法更加常用。</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>
<h1 id="调参"><a href="#调参" class="headerlink" title="调参"></a>调参</h1><p>大多数学习算法都有些参数(parameter)需要设定，参数配置不同，学得模型的性能往往有显著差别。</p>
<p>学习算法的很多参数是在实数范围内取值，因此，对每种参数取值都训练出模型来是不可行的。常用的做法是：对每个参数选定一个范围和步长λ，这样使得学习的过程变得可行。</p>
<div class="note danger simple"><p>当选定好模型和调参完成后，我们需要使用初始的数据集D重新训练模型，即让最初划分出来用于评估的测试集也被模型学习，增强模型的学习效果。</p>
</div>
<hr>
<h1 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h1><p>衡量模型泛化能力的评价标准，在对比<strong>不同模型的能力</strong>时，使用不同的性能度量往往会导致不同的评判结果。</p>
<p>给定样例集$ D={(x_1,y_1),(x_2,y_2),\ldots,(x_m,y_m)} $,其中$y_i$是对示例$x_i$的真实标记，预测结果为$f(x)$</p>
<h2 id="回归任务"><a href="#回归任务" class="headerlink" title="回归任务"></a>回归任务</h2><p>均方误差</p>
<script type="math/tex; mode=display">
E(f;D)=\frac{1}{m}\sum_{i=1}^m{\left( f\left(x_i\right) -y_i\right)}^2</script><p>更一般的，数据分布D和概率密度函数$p(\cdot)$</p>
<script type="math/tex; mode=display">
E(f;D)=\int_{x \in D}\left(f(x)-y\right)^2p(x)dx</script><h2 id="分类任务"><a href="#分类任务" class="headerlink" title="分类任务"></a>分类任务</h2><div class="tabs" id="分类任务的性能度量"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分类任务的性能度量-1">错误率与精度</button></li><li class="tab"><button type="button" data-href="#分类任务的性能度量-2">查准率、查全率和F1</button></li><li class="tab"><button type="button" data-href="#分类任务的性能度量-3">ROC与AUC</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分类任务的性能度量-1"><p>错误率：分类错误的样本占样本总数的比例</p>
<script type="math/tex; mode=display">
E(f;D)=\frac{1}{m}\sum_{i=1}^mⅡ\left(f(x_i)\not=y_i\right)\tag{Ⅱ(true)=1}</script><p>精度：分类正确的样本占样本总数的比例</p>
<script type="math/tex; mode=display">
acc(f;D)=1-E(f;D)</script><p>更一般的，对于数据分布D和概率密度函数$p(·)$</p>
<script type="math/tex; mode=display">
E(f;D)=\int_{s\in D}Ⅱ\left(f\left(x\right)\not=y\right)p(x)dx</script><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分类任务的性能度量-2"><p>分类结果的混淆矩阵</p>
<table>
    <tr>
        <td rowspan="2" align="center" vlign="center">真实情况</td>    
        <td colspan="2" align="center">预测情况</td>  
    </tr>
    <tr>
        <td align="center">正例</td>
        <td align="center">反例</td>
    </tr>
    <tr>
        <td align="center">正例</td>
        <td align="center">TP(真正例)</td>
        <td align="center">FN(假反例)</td>
    </tr>
    <tr>
        <td align="center">反例</td>
        <td align="center">FP(假正例)</td>
        <td align="center">TN(真反例)</td>
    </tr>
</table>

<p><strong>查准率</strong>P：推送给用户的内容用户是否感兴趣。</p>
<p><strong>查全率</strong>R：所有用户感兴趣的内容中，我们推送出来了多少。</p>
<div class="tabs" id="分类划分"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分类划分-1">二分类</button></li><li class="tab"><button type="button" data-href="#分类划分-2">多分类</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分类划分-1"><script type="math/tex; mode=display">
P=\frac{TP}{TP+FP}\tag{查准率}</script><script type="math/tex; mode=display">
R=\frac{TP}{TP+FN}\tag{查全率}</script><p>查全率与查准率时一对矛盾的度量</p>
<p>“P-R曲线”正是描述查准/查全率变化的曲线，P-R曲线定义如下：对测试样本进行排序，将最可能是“正例”的样本排在前面，最不可能是“正例”的排在后面，设置不同的阈值，每次计算出当前阈值下对应的P值和R值，最后连成曲线，如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/lyb92nlf/pictures/master/img/202111141456636.jpg" alt="P-R曲线" style="zoom:50%;" /></p>
<p>图像中有如下信息：</p>
<ul>
<li>模型B完全包含模型C，则B的性能优于C</li>
<li>模型A和模型B出现交叉，则比较曲线下面积的大小，面积大者优</li>
</ul>
<p>但由于面积值不容易估算，则引入平衡点BEP（Break-Even Point），该点处R=P。那么我们认为学习器A优于B。</p>
<p>但BEP过于简化了，更常用F1度量。它是基于查准率与查全率的调和平均：</p>
<script type="math/tex; mode=display">
\frac{1}{F1}=\frac{1}{2}\left(\frac{1}{P}+\frac{1}{R}\right)</script><script type="math/tex; mode=display">
F1=\frac{2\times P\times R}{P+R}=\frac{2\times TP}{样例总数+TP-TN}</script><p>更一般的情况，使用$F_\beta$形式。它是基于于查准率与查全率的加权调和平均：</p>
<script type="math/tex; mode=display">
\frac{1}{F_\beta}=\frac{1}{1+\beta^2}\left(\frac{1}{P}+\frac{\beta^2}{R}\right)</script><script type="math/tex; mode=display">
F_\beta=\frac{\left(1+\beta^2\right)\times P\times R}{\left(\beta^2\times P\right)+R}</script><script type="math/tex; mode=display">
\beta\begin{cases}
> 1,对R有更大的影响\\
< 1,对P有更大的影响\\
= 1,退化为F1
\end{cases}</script><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分类划分-2"><p>多分类问题实际像就是n个二分类问题。每两两类别的组合对应一个混淆矩阵</p>
<div class="tabs" id="多分类问题的度量"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#多分类问题的度量-1">宏观计算</button></li><li class="tab"><button type="button" data-href="#多分类问题的度量-2">微观计算</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="多分类问题的度量-1"><p>先计算，后求均值</p>
<script type="math/tex; mode=display">
macro-P=\frac{1}{n}\sum_{i=1}^nP_i</script><script type="math/tex; mode=display">
macro-R=\frac{1}{n}\sum_{i=1}^nR_i</script><script type="math/tex; mode=display">
macro-F1=\frac{2\times macro-P\times macro-R}{macro-P+macro-R}</script><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="多分类问题的度量-2"><p>先求均值，在计算</p>
<script type="math/tex; mode=display">
micro-P=\frac{\overline{TP}}{\overline{TP}+\overline{FP}}</script><script type="math/tex; mode=display">
micro-R=\frac{\overline{TP}}{\overline{TP}+\overline{FN}}</script><script type="math/tex; mode=display">
micro-F1=\frac{2\times micro-P\times micro-R}{micro-P+macro-R}</script><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分类任务的性能度量-3"><p>学习器对测试样本的评估结果一般为一个实值或概率预测。然后设定一个阈值，大于阈值为正例，小于阈值为负例，因此这个实值或预测结果的好坏，直接决定了学习器的泛化能力。实际上，将这些实值或概率预测进行排序，则排序的好坏决定了学习器的性能高低。</p>
<p>ROC曲线正是从这个角度出发来研究学习器的泛化性能，ROC曲线与P-R曲线相似，根据学习器的预测结果，对样例进行排序，按照排序的顺序逐个把样本作为正例进行预测，每次计算出两个重要量的值，分别以它们为横、纵坐标作图，就得到ROC曲线。不同的是ROC曲线以“真正例率”（True Positive Rate，简称TPR）为纵轴，横轴为“假正例率”（False Positive Rate，简称FPR），两者定义如下：</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>
]]></content>
      <categories>
        <category>笔记</category>
        <category>西瓜书</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>西瓜书</tag>
      </tags>
  </entry>
  <entry>
    <title>决策树</title>
    <url>/2021/11/16/Chapter04%E5%86%B3%E7%AD%96%E6%A0%91/</url>
    <content><![CDATA[<h2 id="基本流程"><a href="#基本流程" class="headerlink" title="基本流程"></a>基本流程</h2><p>决策树是基于<strong>树结构</strong>来进行决策的，例如在西瓜问题中，对新样本的分类可看作对“当前样本属于正类吗”这个问题的“决策”过程。</p>
<p><img src="https://raw.githubusercontent.com/lyb92nlf/pictures/master/img/202111131749669.png" alt="西瓜问题决策树" style="zoom:50%;" /></p>
<p>决策树学习的目的是为了产生一颗泛化能力强，即处理未见示例能力强的决策树。</p>
<p>其基本算法如下图所示</p>
<p><img src="https://raw.githubusercontent.com/lyb92nlf/pictures/master/img/202111131749483.png" alt="决策树学习基本算法" style="zoom:50%;" /></p>
<p>注：在决策树基本算法中，有三种情形导致递归返回</p>
<ol>
<li>当前结点包含的样本全属于同一类别，无需划分 </li>
<li>当前属性集为空，或是所有样本在所有属性上取值相同，无法划分 </li>
<li>当前结点包含的样本集合为空，不能划分。</li>
</ol>
<p>在第2种情形下，把当前结点标记为叶结点，但类别设定为该结点所含样本最多的类别。</p>
<p>在第3种情形下，把当前结点标记为叶节点，但类别设定为其父结点所含样本最多的类别。</p>
<p>它们的不同点是：第2种是利用当前结点的<strong>后验分布</strong>，第3种则是把父结点的样本分布作为当前结点的<strong>先验分布</strong></p>
<h2 id="划分选择"><a href="#划分选择" class="headerlink" title="划分选择"></a>划分选择</h2><p>决策数学习的关键就是如何选择最优划分属性</p>
<ol>
<li><p>信息增益</p>
<p>实战：ID3决策树学习算法</p>
<p>最优划分属性：</p>
<script type="math/tex; mode=display">
a_*=arg\,\max_{a\in A}Gain(D,a)</script><p>公式：</p>
<script type="math/tex; mode=display">
Ent(D)=-\sum_{k=1}^{|y|}p_k\log_2^{p_k}</script><script type="math/tex; mode=display">
Gain(D,a)=Ent(D)-\sum_{v=1}^V\frac{|D^v|}{D}Ent(D^v)</script></li>
</ol>
<ol>
<li><p>增益率</p>
<p>实战：C4.5决策树算法</p>
<ol>
<li>先从候选属性中找出信息增益高于平均水平的属性</li>
<li>在从中选择信息增益最高的属性</li>
</ol>
<p>最优划分属性：</p>
<script type="math/tex; mode=display">
a_*=arg\,\max_{a\in A}Gain\_ratio(D,a)</script><p>公式：</p>
<script type="math/tex; mode=display">
IV(a)=-\sum_{v=1}^V\frac{|D^v|}{|D|}\log_2^{\frac{|D^v|}{|D|}}</script><script type="math/tex; mode=display">
Gain\_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}</script></li>
</ol>
<ol>
<li><p>基尼指数</p>
<p>实战：CART决策树</p>
<p>最优划分属性：</p>
<script type="math/tex; mode=display">
a_*=arg\, \min_{a\in A}\, Gini\_index(D,a)</script><p>公式：</p>
<script type="math/tex; mode=display">
Gini(D)=\sum_{k=1}^{|y|}\sum_{k'≠k}p_kp_{k'}=1\, -\, \sum_{k=1}^{|y|}p_k^2</script><script type="math/tex; mode=display">
Gini\_index(D,a)=\sum_{v=1}^V\frac{|D^v|}{|D|}Gini(D^v)</script></li>
</ol>
<h2 id="剪枝处理"><a href="#剪枝处理" class="headerlink" title="剪枝处理"></a>剪枝处理</h2><p>剪枝是决策树学习算法<strong>对付过拟合</strong>的主要手段。</p>
<ol>
<li><p>预剪枝</p>
<p>对每个结点在<strong>划分前先进行评估</strong>，如果当前节点的划分<strong>不能</strong>带来决策树泛化<strong>性能提升</strong>，则<strong>停止划分</strong>并将当前节点<strong>标记为叶节点</strong></p>
</li>
<li><p>后剪枝</p>
<p>先生成<strong>完整的决策树</strong>，<strong>自底向上</strong>的对<strong>非叶节点</strong>进行考察，如果当前节点对应的子树替换成叶节点能带来决策树泛<strong>化性能提升</strong>，则将子树<strong>替换</strong>成叶节点</p>
</li>
</ol>
<p>对比预剪枝与后剪枝生成的决策树：</p>
<p>后剪枝通常比预剪枝保留更多的分支，其欠拟合风险很小，因此后剪枝的泛化性能往往优于预剪枝决策树。但后剪枝过程是生成完整决策树之后自底往上裁剪，因此其训练时间开销比预剪枝要大。</p>
<h2 id="连续和缺失值"><a href="#连续和缺失值" class="headerlink" title="连续和缺失值"></a>连续和缺失值</h2><h3 id="连续值处理"><a href="#连续值处理" class="headerlink" title="连续值处理"></a>连续值处理</h3><p>使用<strong>连续属性离散化技术</strong>对连续值进行处理</p>
<p>C4.5决策树算法采用二分法处理连续属性</p>
<p>假设连续属性a的值从小到大排序为{a1,a2,a3,…,an}</p>
<p>取划分点（n个值对应n-1个划分点）</p>
<script type="math/tex; mode=display">
T_a = \left\{ \frac{a^i\, + a^{i+1}}{2}\, |\,1≤i≤n-1| \right\}</script><script type="math/tex; mode=display">
\begin{align}
Gain(D,a) &= \max_{t\in T_a}\, Gain(D,a,t)\\
&=\max_{t\in T_a}\, Ent(D)-\sum_{\lambda\in \lbrace -,+\rbrace}\frac{|D^v|}{D}Ent(D_t^\lambda)
\end{align}</script><p>$ D^- $ ：属性值小于t的样本</p>
<p>$D^+$：属性值大于t的样本</p>
<h3 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h3><p>现实任务中常会遇到不完整的样本，即样本的某些属性值缺失</p>
<p>给定数据集D和属性a</p>
<ol>
<li>$\stackrel{*}{D}$为属性a上没有缺失值的样本子集</li>
<li>$\stackrel{*}{D^v}$为在<code>1</code>中属性a上取值为$a^v$的样本子集</li>
<li>$\stackrel{*}{D^k}$表示在<code>1</code>中属于第k类的样本子集</li>
</ol>
<p>于是有</p>
<script type="math/tex; mode=display">
\stackrel{*}{D}=\bigcup_{k=1}^{|y|}\stackrel{*}{D^k},\stackrel{*}{D}=\bigcup_{v=1}^{V}\stackrel{*}{D^v}</script><p>并给每一个样本x赋予一个权重$w_x$（一般初始化全为1）</p>
<p>无缺省样本所占比例</p>
<script type="math/tex; mode=display">
\rho=\frac{\sum_{x\in\stackrel{*}{D}}w_x}{\sum_{x\in D}w_x}</script><p>无缺省样本中第k类所占的比例</p>
<script type="math/tex; mode=display">
\stackrel{*}{p_k}=\frac{\sum_{x\in\stackrel{*}{D_k}}w_x}{\sum_{x\in \stackrel{*}{D}}w_x}</script><p>无缺省样本中属性值为$a^v$所占的比例</p>
<script type="math/tex; mode=display">
\stackrel{*}{r_v}=\frac{\sum_{x\in\stackrel{*}{D_v}}w_x}{\sum_{x\in \stackrel{*}{D}}w_x}</script><p>显然</p>
<script type="math/tex; mode=display">
\sum_{k=1}^{|y|}\stackrel{*}{p_k}=\sum_{v=1}^V\stackrel{*}{r_k}=1</script><div class="note danger simple"><p>问题1：如何在属性值缺失的情况下进行划分属性选择</p>
</div>
<p>于是有新的信息增益公式</p>
<script type="math/tex; mode=display">
\begin{align}
Gain(D,a)&=\rho\times Gain(\stackrel{*}{D},a)\\
&=\rho\times\left( Ent\left(\stackrel{*}{D}\right)-\sum_{v=1}^V\stackrel{*}{r_v}Ent\left(\stackrel{*}{D^v}\right)\right)
\end{align}</script><p>其中</p>
<script type="math/tex; mode=display">
Ent(\stackrel{*}{D})=-\sum_{k=1}^{|y|}\stackrel{*}{p_k}\log_2^{\stackrel{*}{p_k}}</script><div class="note danger simple"><p>问题2：给定划分属性，如果样本在该属性上的值缺失，如何对样本进行划分</p>
</div>
<ol>
<li>样本x在划分属性a上的取值已知，则将x划入取值对应的子节点，样本权重不变</li>
<li>样本x在划分属性a上的取值未知，则将x同时划入所有子节点，样本权重在属性值为$a^v$调整为$\stackrel{*}{r_v}\cdot w_x$</li>
</ol>
<h2 id="多变量决策树"><a href="#多变量决策树" class="headerlink" title="多变量决策树"></a>多变量决策树</h2><p>此类决策树中，非叶节点不在是仅对某个属性，而是对属性的线性组合进行测试。</p>
<p>即每个非叶节点是一个形如$\sum_{i=1}^{d}w_ia_i=t$的线性分类器，其中$w_i$是属性$a_i$的权重，$w_i$和$t$可在该节点所含样本的样本集和属性集学到</p>
]]></content>
      <categories>
        <category>笔记</category>
        <category>西瓜书</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>西瓜书</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络</title>
    <url>/2021/11/17/Chapter05%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<h1 id="M-P神经元模型"><a href="#M-P神经元模型" class="headerlink" title="M-P神经元模型"></a>M-P神经元模型</h1><p>在这个模型中，神经元收到来自n个神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，神经元收到的总输入值与神经元的阈值进行比较，然后通过激活函数处理以产生神经元的输出</p>
<p><img src="https://raw.githubusercontent.com/lyb92nlf/pictures/master/img/202111171044734.jpg" alt="神经元模型" style="zoom:50%;" /></p>
<p>常见的激活函数</p>
<script type="math/tex; mode=display">
sgn(x)=\begin{cases}
1,x\geq0\\
0,x < 0
\end{cases}
\;\;\;\;\;\;\;\;\;sigmoid(x)=\frac{1}{1+e^{-x}}</script><h1 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h1><p>包含输入层和输出层，输出层是M-P神经元</p>
<p>感知机非常容易实现逻辑与、或和非运算（线性可分问题：存在一个线性超平面将两种模式分开）。</p>
<h1 id="多层前馈神经网络"><a href="#多层前馈神经网络" class="headerlink" title="多层前馈神经网络"></a>多层前馈神经网络</h1><p>解决非线性可分问题，往往需要多层网络。</p>
<p>多层前馈神经网络，通常分为输入层，隐层和输出层。隐层和输出层的神经元是拥有激活函数的功能神经元。同时，每层的神经元都与下一层的神经元全互联。</p>
<p>神经网络的学习过程，就是根据训练数据来调整神经元之间的连接权值和每个功能神经元的阈值</p>
<h1 id="误差逆传播算法"><a href="#误差逆传播算法" class="headerlink" title="误差逆传播算法"></a>误差逆传播算法</h1><p>BP算法，也称为反向传播算法</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：训练集D，学习率</span><br><span class="line">过程：</span><br><span class="line">1：在(0,1)范围内随机初始化网络中所有的连接权值和阈值</span><br><span class="line">repeat</span><br><span class="line">	for all (x,y)∈D do</span><br><span class="line">		根据当前参数的和式计算当前样本的输出；</span><br><span class="line">		计算隐层神经元的梯度项；</span><br><span class="line">		计算输入神经元的梯度项；</span><br><span class="line">		更新连接权值和阈值；</span><br><span class="line">        end for</span><br><span class="line">until 达到停止条件</span><br><span class="line">输出：连接权值和阈值确定的多层前馈神经网络</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/lyb92nlf/pictures/master/img/202111171141160.jpg" alt="多层前馈神经网络" style="zoom:50%;" /></p>
<ol>
<li>计算当前参数的和式计算当前样本的输出，f=sigmoid</li>
</ol>
<p>​    </p>
<script type="math/tex; mode=display">
\hat{y}_j^k=f(\beta_j-\theta_j)</script><ol>
<li><p>计算隐层神经元的梯度项</p>
<p>第k个样本的均方误差</p>
<script type="math/tex; mode=display">
E_k=\frac{1}{2}\sum_{j=1}^l(\hat{y}_j^k-y_j^k)^2</script><script type="math/tex; mode=display">
\Delta w_{hj}=-\eta\frac{\partial E_K}{\partial w_{hj}}=\eta g_jb_h</script><script type="math/tex; mode=display">
\Delta \theta_j=-\eta\frac{\partial E_k}{\partial \theta_j}=-\eta g_j</script><script type="math/tex; mode=display">
g_j=\hat{y}_j^k(1-\hat{y}_j^k)(y_j^k-\hat{y}_j^k)</script></li>
</ol>
<ol>
<li><p>计算隐层神经元的梯度项</p>
<script type="math/tex; mode=display">
\Delta v_{ih}=\eta e_hx_i</script><script type="math/tex; mode=display">
\Delta \gamma_h=-\eta e_h</script><script type="math/tex; mode=display">
e_h=b_h(1-b_h)\sum_{j=1}^lw_{hj}g_j</script></li>
<li><p>更新连接权值和阈值</p>
<script type="math/tex; mode=display">
v\leftarrow v+\Delta v</script></li>
</ol>
<div class="note warning simple"><p>如何设置隐层神经元的个数？</p>
</div>
<p>只要一个包含足够多神经的隐层，多层前馈神经网络就能以任意精度逼近任意复杂度的连续函数</p>
<p>实际应用中，通常靠“试错”调整</p>
<div class="note warning simple"><p>如何解决BP神经网络过拟合问题？</p>
</div>
<ol>
<li><p>早停：若训练集误差降低，验证集误差升高，则停止训练。返回具有最小验证集误差的连接权值和阈值</p>
</li>
<li><p>正则化：基本思想是在误差目标函数中添加一个描述网络复杂度的部分</p>
<p>如加入连接权值和阈值的平方和</p>
<script type="math/tex; mode=display">
E=\lambda\frac{1}{m}\sum_{k=1}^mE_k+(1-\lambda)\sum_iw_i^2</script></li>
</ol>
<h1 id="全局最小和局部极小"><a href="#全局最小和局部极小" class="headerlink" title="全局最小和局部极小"></a>全局最小和局部极小</h1><p>神经网络的任务是寻找到解的全局最小，然而却常常陷入局部极小解。</p>
<p>常用的“跳出”局部极小的策略有：</p>
<ol>
<li>使用多组不同的参数初始化多个神经网络，按标准方法训练后，取误差最小的解作为最终参数</li>
<li>模拟退火：每一步以一定的概率接受比当前解更差的结果。每步迭代的过程中，接受次最优解的概率逐渐降低</li>
<li>在计算梯度时加入随机因素</li>
</ol>
]]></content>
      <categories>
        <category>笔记</category>
        <category>西瓜书</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>西瓜书</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>集成学习</title>
    <url>/2021/11/25/Chapter08%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h1 id="个体与集成"><a href="#个体与集成" class="headerlink" title="个体与集成"></a>个体与集成</h1><p>集成学习通过构建并结合多个学习器来完成学习任务，获得比单一学习器显著优越的泛化性能。</p>
<ul>
<li>同质集成：个体学习器（<strong>基学习器</strong>）使用同一种学习算法（<strong>基学习算法</strong>）</li>
<li>异质集成：个体学习器（<strong>组件学习器</strong>）使用不同的学习算法</li>
</ul>
<p>其基本结构如下图所示</p>
<p><img src="https://raw.githubusercontent.com/lyb92nlf/pictures/master/img/202111251428444.png" alt="集成学习"></p>
<p>集成学习的很多理论研究基于弱学习器（精度略高于50%），要获得好的集成，个体学习器应该“好而不同”，即个体有一定的精度，同时个体之间存在差异。</p>
<p><img src="https://raw.githubusercontent.com/lyb92nlf/pictures/master/img/202111251431891.png" alt="好而不同" style="zoom:50%;" /></p>
<p>目前集成学习方法大致分为两大类。</p>
<ul>
<li>个体学习器间存在强依赖关系、必须串行生成得序列化方法（Boosting）</li>
<li>个体学习器间不存在强依赖关系、可同时生成得并行化方法（Bagging和随机森林）</li>
</ul>
<h1 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h1><p>工作机制：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使先前做错的训练样本在后续受到更多关注，然后基于调整后的样本分布训练下一个基学习器，如此重复，直到基学习器达到指定的数目T，最终将T个基学习器加权结合。</p>
<p><img src="https://raw.githubusercontent.com/lyb92nlf/pictures/master/img/202111251451993.png" alt="工作机制" style="zoom: 50%;" /></p>
<p>代表性算法：Adaboost</p>
<p><img src="https://raw.githubusercontent.com/lyb92nlf/pictures/master/img/202111251512537.jpg" alt="adaboost"></p>
<h1 id="Bagging与随机森林"><a href="#Bagging与随机森林" class="headerlink" title="Bagging与随机森林"></a>Bagging与随机森林</h1><h2 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h2><p>使用自助采样法采样出T个包含m个训练样本的采样集，然后基于每个采样集训练出一个学习器，再将这些学习器结合起来。</p>
<ul>
<li>分类任务：使用简单投票法。票数一致，则随机选择一个</li>
<li>回归任务：使用简单平均法</li>
</ul>
<p>从偏差-方差分解的角度看，Bagging主要关注降低方差，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效果更明显</p>
<h2 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h2><p>随机森林是Bagging的一个扩展变体，是以决策树为及学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入随机属性选择。具体来说，传统决策树在选择划分属性时是在当前结点的属性集合(假定有d 个属性)中选择一个最优属性;而在RF 中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分. 这里的参数k 控制了随机性的引入程度;若令k = d k = dk=d, 则基决策树的构建与传统决策树相同;若令k = 1 ， 则是随机选择一个属性用于划分; 一般情况下，推荐值$k=\log_2^d$</p>
<h2 id="结合策略"><a href="#结合策略" class="headerlink" title="结合策略"></a>结合策略</h2><h2 id="平均法-回归任务"><a href="#平均法-回归任务" class="headerlink" title="平均法 - 回归任务"></a>平均法 - 回归任务</h2><ol>
<li><p>简单平均法</p>
<p>所有个体学习器输出的平均值</p>
</li>
<li><p>加权平均法</p>
<p>所有个体学习器输出的加权平均值，权重之和为1</p>
</li>
</ol>
<h2 id="投票法-分类任务"><a href="#投票法-分类任务" class="headerlink" title="投票法 - 分类任务"></a>投票法 - 分类任务</h2><ol>
<li><p>绝对多数投票法</p>
<p>若某标记得票超过一半，则预测为该标记。否则，拒绝预测</p>
</li>
<li><p>相对多数投片法</p>
<p>预测为得票数最多的标记。若有多个，则随机选择一个</p>
</li>
<li><p>加权投票法</p>
<script type="math/tex; mode=display">
H(x)=c_{\underset{j}{argmax}\sum_{t=1}^Tw_ih_i^j(x)}</script></li>
</ol>
<h2 id="学习法"><a href="#学习法" class="headerlink" title="学习法"></a>学习法</h2><p>当训练数据很多时，一种更为强大的结合策略就是“学习法”，即通过一个学习器来进行结合。Stacking是学习法的典型代表。Stacking 先从初始数据集训练出初级学习器（T个个体学习器），然后”生成”一个新数据集（个体学习器的输出）用于训练次级学习器。</p>
<p><img src="https://raw.githubusercontent.com/lyb92nlf/pictures/master/img/202111251537855.png" alt="学习法" style="zoom:50%;" /></p>
<p>为了防止过拟合，一般采用交叉验证的方法，用训练初级学习器未使用的样本产生次级学习器的训练样本</p>
<p><img src="https://raw.githubusercontent.com/lyb92nlf/pictures/master/img/202111251630238.png" alt="stacking举例"></p>
<h1 id="多样性"><a href="#多样性" class="headerlink" title="多样性"></a>多样性</h1><h2 id="多样性度量"><a href="#多样性度量" class="headerlink" title="多样性度量"></a>多样性度量</h2><p>用于度量集成中个体学习器的多样性，即估算个体学习器的多样化程度</p>
<p>给定数据集|D|=m，对于二分类任务</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">h1=+1</th>
<th style="text-align:center">h1=-1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">h2=+1</td>
<td style="text-align:center">a</td>
<td style="text-align:center">b</td>
</tr>
<tr>
<td style="text-align:center">h2=-1</td>
<td style="text-align:center">c</td>
<td style="text-align:center">d</td>
</tr>
</tbody>
</table>
</div>
<p>其中，h代表不同的个体学习器，a+b+c+d=m</p>
<div class="tabs" id="多样化度量"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#多样化度量-1">1.不合度量</button></li><li class="tab"><button type="button" data-href="#多样化度量-2">2.相关系数</button></li><li class="tab"><button type="button" data-href="#多样化度量-3">3.Q-统计量</button></li><li class="tab"><button type="button" data-href="#多样化度量-4">4.k-统计量</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="多样化度量-1"><script type="math/tex; mode=display">
dis_{ij}=\frac{b+c}{m}\in[0,1]\tag{值与多样性成正比}</script><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="多样化度量-2"><script type="math/tex; mode=display">
\rho_{ij}=\frac{ad-bc}{\sqrt{(a+b)(a+c)(c+d)(b+d)}}\in[-1,1]</script><p>h无关，值为0；正相关，值为正；负相关，值为负</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="多样化度量-3"><script type="math/tex; mode=display">
Q_{ij}=\frac{ad-bc}{ad+bc}</script><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="多样化度量-4"><script type="math/tex; mode=display">
k=\frac{p_1-p_2}{1-p_2}\;\;\;p_1=\frac{a+d}{m}\;\;\;p_2=\frac{(a+b)(a+c)+(b+d)(c+d)}{m^2}</script><p>p1为分类器取得一致的概率，p2为分类器偶尔达成一致的概率</p>
<p>常见的“k-误差图”，横轴一对分类器k统计量，纵轴为平均误差。集成后的数据点云越往上，准确性越低；越往右，多样性越小。</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>
<h2 id="多样性增强"><a href="#多样性增强" class="headerlink" title="多样性增强"></a>多样性增强</h2><p>常见的作法是对数据样本、输入属性、算法参数进行扰动。</p>
]]></content>
      <categories>
        <category>笔记</category>
        <category>西瓜书</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>西瓜书</tag>
        <tag>继承学习</tag>
        <tag>adaboost</tag>
        <tag>随机森林</tag>
      </tags>
  </entry>
  <entry>
    <title>聚类</title>
    <url>/2021/11/30/Chapter09%E8%81%9A%E7%B1%BB/</url>
    <content><![CDATA[<h1 id="聚类任务"><a href="#聚类任务" class="headerlink" title="聚类任务"></a>聚类任务</h1><p>在无监督学习过程中，训练样本的标记信息是未知的，目标是通过对无标记训练样本的学习揭示数据的内在性质和规律。此类学习任务中研究最多、应用最广的就是<strong>“聚类”</strong></p>
<p>存在无标记的训练样本集D，聚类算法会将D划分为k个互不相交的<strong>簇</strong>。聚类过程只能自动形成簇，簇所对应的概念语义（即<strong>簇标记</strong> - 训练时不知道）需由使用者来把握和命名。</p>
<p>聚类算法中会涉及两个基本问题——性能度量和距离计算</p>
<h2 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h2><p>性能度量的作用</p>
<ol>
<li>用于评估聚类结果的好坏</li>
<li>明确最终的性能度量，可以直接作为聚类过程中的优化目标</li>
</ol>
<p>定义数据集D（m个d维X样本），聚类的簇划分为C（k个簇），参考模型的簇划分为C*（s个簇），λ和λ*分别对应为不同簇划分的簇标记</p>
<p>性能度量的分类</p>
<div class="tabs" id="性能度量的分类"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#性能度量的分类-1">外部指标</button></li><li class="tab"><button type="button" data-href="#性能度量的分类-2">内部指标</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="性能度量的分类-1"><p>定义：将聚类结果与某个“参考模型”进行比较</p>
<p>将D中样本两两配对，a+b+c+d=m(m-1)/2</p>
<script type="math/tex; mode=display">
\begin{align}
&a=|SS|,SS=\{(x_i,x_j)|\lambda_i=\lambda_j,\lambda_i^*=\lambda_j^*,i<j\}\nonumber\\
&b=|SD|,SD=\{(x_i,x_j)|\lambda_i=\lambda_j,\lambda_i^*\not=\lambda_j^*,i<j\}\nonumber\\
&c=|DS|,DS=\{(x_i,x_j)|\lambda_i\not=\lambda_j,\lambda_i^*=\lambda_j^*,i<j\}\nonumber\\
&d=|DD|,DD=\{(x_i,x_j)|\lambda_i\not=\lambda_j,\lambda_i^*\not=\lambda_j^*,i<j\}\nonumber\\
\end{align}</script><ul>
<li>Jaccard指数<script type="math/tex; mode=display">
JC=\frac{a}{a+b+c}</script></li>
</ul>
<ul>
<li>FM指数<script type="math/tex; mode=display">
FMI=\sqrt{\frac{a}{a+b}\times\frac{a}{a+c}}</script></li>
</ul>
<ul>
<li>Rand指数</li>
</ul>
<p>​    </p>
<script type="math/tex; mode=display">
RI=\frac{2(a+d)}{m(m-1)}</script><p>以上性能度量结果均在[0,1]内，值越大越好</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="性能度量的分类-2"><p>直接考察聚类结果，不考虑其他模型，C表示簇划分，dist表示样本之间的距离，u表示簇Ci的中心点。</p>
<p>avg表示簇Ci内样本的平均距离，diam表示簇C内样本的最大距离，dmin表示最小距离，dcen表示不同簇中心点的距离</p>
<script type="math/tex; mode=display">
\begin{align}
&avg(C_k)=\frac{2}{|C_k|(|C_k|-1)}\sum_{1\leq i \leq j \leq|C_k|}dist(x_i,x_j)\nonumber\\
&diam(C_k)=\max_{1\leq i \leq j \leq |C_k|}dist(x_i,x_j)\nonumber\\
&d_{min}(C_i,C_j)=\min_{x_i\in C_i,x_j\in C_j}dist(x_i,x_j)\nonumber\\
&d_{cen}(C_i,C_j)=dist(\mu_i,\mu_j)\nonumber \\
&\mu_i=\frac{1}{|C_i|}\sum_{j\in C_i}x_j
\end{align}</script><ul>
<li>DB指数<script type="math/tex; mode=display">
DBI=\frac{1}{m}\sum_{i=1}^k\max_{j\not=i}\left(\frac{avg(C_i)+avg(C_j)}{d_{cen}(\mu_i,\mu_j)}\right)</script></li>
</ul>
<ul>
<li>Dunn指数<script type="math/tex; mode=display">
DI=\min_{1\leq i \leq k}\left\{\min_{j\not=i}\left(\frac{d_{min}(C_i,C_j)}{\max_{1\leq l \leq k }diam(C_l)}\right) \right\}</script></li>
</ul><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>
<h2 id="距离计算"><a href="#距离计算" class="headerlink" title="距离计算"></a>距离计算</h2><p>对于dist(·,·)，需要满足以下性质</p>
<ol>
<li>非负（≥0）</li>
<li>同一性（当且仅当x=y，有dist(x,y)=0）</li>
<li>对称性（dist(x,y)=dist(y,x)）</li>
<li>直递性（dist(x,z)≤dist(x,y)+dist(y,z)）</li>
</ol>
<h3 id="对于有序属性（可以直接进行数值计算的）"><a href="#对于有序属性（可以直接进行数值计算的）" class="headerlink" title="对于有序属性（可以直接进行数值计算的）"></a>对于有序属性（可以直接进行数值计算的）</h3><div class="tabs" id="距离"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#距离-1">闵可夫斯基距离</button></li><li class="tab"><button type="button" data-href="#距离-2">欧式距离</button></li><li class="tab"><button type="button" data-href="#距离-3">曼哈顿距离</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="距离-1"><script type="math/tex; mode=display">
dist_{mk}(x_i,x_j)=\left(\sum_{u=1}^n|x_{iu}-x_{ju}|^p\right)^{\frac{1}{p}}\tag{p≥1}</script><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="距离-2"><script type="math/tex; mode=display">
dist_{ed}(x_i,x_j)=\left(\sum_{u=1}^n|x_{iu}-x_{ju}|^2\right)^{\frac{1}{2}}\tag{p=2}</script><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="距离-3"><script type="math/tex; mode=display">
dist_{man}(x_i,x_j)=\sum_{u=1}^n|x_{iu}-x_{ju}|\tag{p=1}</script><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>
<h3 id="对于无序属性（不能直接进行数值计算的）"><a href="#对于无序属性（不能直接进行数值计算的）" class="headerlink" title="对于无序属性（不能直接进行数值计算的）"></a>对于无序属性（不能直接进行数值计算的）</h3><p>属性u上两个离散值a，b之间的距离</p>
<script type="math/tex; mode=display">
VDM_p(a,b)=\sum_{i=1}^{样本簇数}\left|\frac{第i个样本簇中属性u取值为a的样本数}{属性u上取值为a的样本数}-\frac{第i个样本簇中属性u取值为b的样本数}{属性u上取值为b的样本数}\right|^p</script><h3 id="对于混合属性"><a href="#对于混合属性" class="headerlink" title="对于混合属性"></a>对于混合属性</h3><p>假设有nc个有序属性，n-nc个无序属性，则</p>
<script type="math/tex; mode=display">
MinkovDM_p(x_i,x_j)=\left(\sum_u^{n_c}|x_{iu}-x_{ju}|^p+\sum_{u=n_c+1}^nVDM_p(x_{iu},x_{ju})\right)^\frac{1}{p}</script><p>当样本空间内不同属性的重要性不同时，可以使用<strong>加权距离</strong>，在每一项前添加不同的权重</p>
<p>相似度度量：基于某种形式的距离</p>
<p>非度量距离：相似度度量的距离不一定满足距离度量的所有性质</p>
<p><img src="https://raw.githubusercontent.com/lyb92nlf/pictures/master/img/202111301634768.png" alt="非度量距离"></p>
]]></content>
      <categories>
        <category>笔记</category>
        <category>西瓜书</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>西瓜书</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title>贝叶斯分类器</title>
    <url>/2021/11/22/Chapter07%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/</url>
    <content><![CDATA[<blockquote>
<p>可以先学习<a href="https://zhuanlan.zhihu.com/p/32825019">贝叶斯公式</a>和<a href="https://zhuanlan.zhihu.com/p/26262151">一个简单的朴素贝叶斯分类算法</a></p>
</blockquote>
<h1 id="贝叶斯决策论"><a href="#贝叶斯决策论" class="headerlink" title="贝叶斯决策论"></a>贝叶斯决策论</h1><p>是概率框架下实施决策的基本方法，以多分类任务解释其基本原理</p>
<p>定义样本x的风险</p>
<script type="math/tex; mode=display">
R(c_i|x)=\sum_{j=1}^N\lambda_{ij}P(c_j|x)</script><p>大概意思就是：基于x的特征判定为某类别的概率 × 判定为该类别的损失λ 之和</p>
<p>任务：使所有样本x的风险最小。</p>
<p>贝叶斯判定准则：为最小化总体风险，只需每个样本选择能使风险最小的类别</p>
<p><br/></p>
<p>具体来说，定义损失λ为：正确类别（即i=j），损失为0；不正确类别，损失为1</p>
<p>那么，样本x的风险=1-基于x的特征判定为某类别的概率</p>
<p>所以最小化风险的贝叶斯最优分类器为<strong>基于x的特征判定为某类别的概率最大</strong></p>
<p><strong>即对于每个样本x，选择能使基于x的特征判定概率最大的类别</strong></p>
<script type="math/tex; mode=display">
p(c|x)=\frac{p(c)p(x|c)}{p(x)}</script><p>关键就是<strong>类条件概率p(x|c)</strong>的求取</p>
<h2 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h2><blockquote>
<p>参考<a href="https://www.matongxue.com/madocs/447/">马同学 (matongxue.com)</a>，极大似然估计就是通过事实，推断出最有可能的参数</p>
</blockquote>
<p>估计类条件概率的一种常用策略：</p>
<ol>
<li>先假设类条件概率具有某种确定的概率分布形式</li>
<li>再基于训练集对概率分布的参数进行估计。</li>
</ol>
<p>假设p(x|c)，x为多维向量</p>
<ol>
<li>具有确定的形式，且被多个参数唯一确定</li>
<li>使用训练集D对多个参数进行估计（设定参数，使得所有的x出现的概率最大）</li>
<li>通过估计的参数求出各类别的p(x|c)</li>
</ol>
<p>实际应用中，概率分布形式应利用应用任务本身的经验知识，如果只是猜测，很可能产生误导性结果</p>
<h2 id="朴素贝叶斯分类器"><a href="#朴素贝叶斯分类器" class="headerlink" title="朴素贝叶斯分类器"></a>朴素贝叶斯分类器</h2><script type="math/tex; mode=display">
p(类别|特征)=\frac{p(类别)p(特征|类别)}{p(特征)}</script><p>前提：假设样本所有属性相互独立</p>
<script type="math/tex; mode=display">
p(c|x)=\frac{p(c)p(x|c)}{p(x)}=\frac{p(c)}{p(x)}\prod_{i=1}^dp(x_i|c)</script><p>由于每个样本的p(x)相等，所以，朴素贝叶斯分类器的表达式为</p>
<script type="math/tex; mode=display">
h_{nb}(x)=\underset{c\in \gamma}{argmax}\;p(c)\prod_{i=1}^dp(x_i|c)</script><p>对于连续属性的p(xi|c)使用<strong>极大似然估计</strong>，xi为一维向量</p>
<h3 id="拉普拉斯修正"><a href="#拉普拉斯修正" class="headerlink" title="拉普拉斯修正"></a>拉普拉斯修正</h3><p>如果在训练集里好瓜从未出现过敲声清脆的值，那么当测试样本x中出现清脆时，p(清脆|好瓜)=0，p(好瓜|x)=0，这样，无论该样本其他属性哪怕明显是好瓜，分类结果都将是坏瓜。为了避免这种问题，常用<strong>拉普拉斯修正</strong></p>
<p>定义N：训练集D中可能的类别个数，Ni表示第i个属性可能的取值数</p>
<p>则</p>
<script type="math/tex; mode=display">
p(c)=\frac{|D_c|+1}{|D|+N}\;\;\;\;\;p(x_i|c)=\frac{|D_{c,x_i}|+1}{|D_c|+N_i}</script><h1 id="半朴素贝叶斯分类器"><a href="#半朴素贝叶斯分类器" class="headerlink" title="半朴素贝叶斯分类器"></a>半朴素贝叶斯分类器</h1><h1 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h1><p>隐变量：例如由于西瓜的根蒂脱落，无法看出是“蜷缩”或“硬挺”，这种未观察的变量叫做隐变量</p>
<p>以初始值Θ为起点，迭代执行以下步骤直至收敛：</p>
<ul>
<li>基于Θ推断隐变量的期望</li>
<li>基于期望和已观测的变量对参数Θ做极大似然估计</li>
</ul>
<h1 id="朴素贝叶斯分类器示例"><a href="#朴素贝叶斯分类器示例" class="headerlink" title="朴素贝叶斯分类器示例"></a>朴素贝叶斯分类器示例</h1><p>假设存在以下训练集</p>
<p><img src="https://raw.githubusercontent.com/lyb92nlf/pictures/master/img/202111221412472.jpg" alt="训练集" style="zoom:80%;" /></p>
<p>测试集：第一行样本</p>
<p>计算过程和测试结果</p>
<p><img src="https://raw.githubusercontent.com/lyb92nlf/pictures/master/img/202111231007713.jpg" alt="过程" style="zoom:80%;" /></p>
<p><img src="https://raw.githubusercontent.com/lyb92nlf/pictures/master/img/202111231007688.jpg" alt="结果" style="zoom:80%;" /></p>
]]></content>
      <categories>
        <category>笔记</category>
        <category>西瓜书</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>西瓜书</tag>
        <tag>贝叶斯分类器</tag>
      </tags>
  </entry>
  <entry>
    <title>SVM数学知识补充</title>
    <url>/2021/11/18/SVM%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86%E8%A1%A5%E5%85%85/</url>
    <content><![CDATA[<h1 id="原问题"><a href="#原问题" class="headerlink" title="原问题"></a>原问题</h1><p>Prime Problem</p>
<p>最小化$f(w)$，w为所有待优化的未知量</p>
<p>限制条件：</p>
<ul>
<li>$g_i(w)\leq0,i\in[1,k]$</li>
<li>$h_i(w)=0,i\in[1,M]$</li>
</ul>
<blockquote>
<p>非常具有普适性：</p>
<ol>
<li>最小化问题f(x)，则-f(x)是最大化问题</li>
<li>限制条件包含大于等于0和等于C的情况</li>
</ol>
</blockquote>
<h1 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h1><p>首先，定义</p>
<script type="math/tex; mode=display">
\begin{align}
L(w,\alpha,\beta)&=f(w)+\sum_{i=1}^k\alpha_ig_i(w)+\sum_{i=1}^M\beta_ih_i(w)\nonumber
\end{align}</script><p>Dual Problem</p>
<p>最大化</p>
<script type="math/tex; mode=display">
\theta(\alpha,\beta)=\underset{所有w}{\inf}\lbrace L(w,\alpha,\beta)\rbrace</script><p>限制条件</p>
<ul>
<li>$\alpha_i\geq0,i\in[0,k]$</li>
</ul>
<blockquote>
<p>inf：求最小值</p>
<p>确定α、β的取值的情况下，遍历所有的w求L最小值</p>
<p>当取不同的α、β的情况下，取最大值θ</p>
</blockquote>
<h1 id="原问题与对偶问题的关系"><a href="#原问题与对偶问题的关系" class="headerlink" title="原问题与对偶问题的关系"></a>原问题与对偶问题的关系</h1><p>定理：</p>
<script type="math/tex; mode=display">
如果w^*是原问题的解，\alpha^*,\beta^*是对偶问题的解，则有
f(w^*)\geq\theta(\alpha^*,\beta^*)</script><p>证明</p>
<script type="math/tex; mode=display">
\begin{align}
\theta(\alpha^*,\beta^*)&=\inf(L(w,\alpha^*,\beta^*))\nonumber\\
&\leq L(w^*,\alpha^*,\beta^*)\nonumber\\
&=f(w^*)+\sum_{i=1}^k\alpha_i^*g_i(w*)+\sum_{i=1}^M\beta_i^*h_i(w^*)\nonumber\\
&\leq f(w^*)\nonumber
\end{align}</script><p>第三步证明过程：</p>
<script type="math/tex; mode=display">
因为w^*,\alpha^*,\beta^*是解，且限制条件\alpha_i^*\geq0,g_i(w*)\leq0,h_i(w^*)=0</script><h1 id="重要结论"><a href="#重要结论" class="headerlink" title="重要结论"></a>重要结论</h1><ol>
<li>原问题与对偶问题的间距<code>Duality Gap</code></li>
</ol>
<script type="math/tex; mode=display">
G=f(w^*)-\theta(\alpha^*,\beta^*)</script><ol>
<li><p>对于某些特定的优化问题，可以证明G=0</p>
</li>
<li><p>强对偶定理</p>
<p>若f(w)为<strong>凸函数</strong>，且g(w)、h(w)为<strong>线性函数</strong>，则此优化问题的G=0</p>
<script type="math/tex; mode=display">
\begin{align}
\theta(\alpha^*,\beta^*)&=\inf(L(w,\alpha^*,\beta^*))\nonumber\\
&=L(w^*,\alpha^*,\beta^*)\nonumber\\
&=f(w^*)+\sum_{i=1}^k\alpha_i^*g_i(w*)+\sum_{i=1}^M\beta_i^*h_i(w^*)\nonumber\\
&=f(w^*)\nonumber
\end{align}</script></li>
</ol>
<blockquote>
<p>第一个≤变成=：w*在α*、β*的条件下是L取到最小值</p>
<p>第二个≤变成=：KKT条件</p>
</blockquote>
<script type="math/tex; mode=display">
a_i^*=0\; 或\; g_i(w^*)=0\tag{对任意的i∈[1,k]}</script><h1 id="凸函数"><a href="#凸函数" class="headerlink" title="凸函数"></a>凸函数</h1><p>必然存在最小值。外国人认为的凸与中国人认为的凸不太一样，下面给出凸函数的定义</p>
<p><img src="https://raw.githubusercontent.com/lyb92nlf/pictures/master/img/202111181607529.png" alt="凸函数" style="zoom: 33%;" /></p>
<p>这里的x,y可以扩展到多维，仍然满足</p>
<script type="math/tex; mode=display">
f(\lambda x+(1-\lambda)y)\leq\lambda f(x)+(1-\lambda)f(y)\;\;\;\lambda\in[0,1]</script>]]></content>
      <categories>
        <category>笔记</category>
        <category>数学知识</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>西瓜书</tag>
        <tag>对偶问题</tag>
        <tag>优化理论</tag>
      </tags>
  </entry>
  <entry>
    <title>支持向量机</title>
    <url>/2021/11/18/Chapter06%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</url>
    <content><![CDATA[<h1 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h1><p>分类学习的基本思想是基于训练集在样本空间中找到一个划分超平面，将不同类别的样本分开。但是划分样本的超平面有很多，怎么找到最好的那一个呢？</p>
<p><img src="https://raw.githubusercontent.com/lyb92nlf/pictures/master/img/202111181659982.jpg" alt="多个超平面" style="zoom:50%;" /></p>
<blockquote>
<p>上图中，直观上“正中间”的超平面是最好的。的确，该超平面产生的分类结果是最鲁棒的，对未见示例的泛化能力最强</p>
<p>其中，x和o表示具有两个属性的样本数据X=(x1;x2)</p>
</blockquote>
<p><code>Vapnik</code>定义了一个性能指标来选择最好的划分超平面。定义一个划分超平面1，不断平移平面直到最先接触到一个正类或一个反类，这样就得到了两个超平面1-1和1-2，这两个超平面之间的距离就作为性能指标。最好的划分超平面就是使<strong>距离(性能指标)最大</strong>的划分超平面。然而，这样的超平面有无数个，那我们就选择1-1和1-2<strong>中间</strong>的超平面。</p>
<h2 id="数学描述"><a href="#数学描述" class="headerlink" title="数学描述"></a>数学描述</h2><ol>
<li><p>训练数据和标签</p>
<script type="math/tex; mode=display">
D=\{(x_i,y_i)\}_{i=1}^N\;\;\;\;y_i\in\{-1,1\}\;\;\;x_i为d维向量</script></li>
<li><p>超平面（类比三维空间容易理解）</p>
<script type="math/tex; mode=display">
w^Tx+b=0</script><p>w为d维列向量，表示超平面的法向量。b是常数，决定超平面与原点的位置。</p>
</li>
<li><p>样本x到超平面的距离（类比点到直线的距离）</p>
<script type="math/tex; mode=display">
r=\frac{|w^Tx+b|}{||w||}</script></li>
<li><p>训练集线性可分的含义</p>
<script type="math/tex; mode=display">
\begin{cases}
w^Tx_i+b\geq0,y_i=1\\
w^Tx_i+b < 0,y_i=-1 
\end{cases}</script><p>即</p>
<script type="math/tex; mode=display">
y_i(w^Tx_i+b)\geq0\tag{公式一}</script></li>
<li><p>间隔（上面说的性能指标 - 距离）</p>
<p><img src="https://raw.githubusercontent.com/lyb92nlf/pictures/master/img/202111181659745.jpg" alt="支持向量与间隔" style="zoom:50%;" /></p>
<p>已知wTx+b=0与awT+ab=0是同一平面，a属于正实数，则利用a进行放缩，使得支持向量x上有</p>
<script type="math/tex; mode=display">
|w^Tx+b|=1</script><p>那么，间隔公式为</p>
<script type="math/tex; mode=display">
r=\frac{2}{||w||}</script><p>有了上述5点的描述，我们便可以引出支持向量机的优化问题（凸优化的二次规划问题）：</p>
<script type="math/tex; mode=display">
\begin{align}
&\underset{w,b}{\max}\frac{2}{||w||}\Leftrightarrow \underset{w,b}{\min}\frac{1}{2}||w||^2\nonumber\\
&s.t. y_i(w^Tx_i+b)\geq1,i=1,2,\dots,N\nonumber
\end{align}</script><blockquote>
<ol>
<li><p>1/2只是为了求导方便</p>
</li>
<li><p>二次规划问题</p>
<ul>
<li>目标函数为二次项</li>
<li>约束条件为一次项</li>
</ul>
<p>那么要么无解，要么只有一个极小值（类似梯度下降）</p>
</li>
</ol>
</blockquote>
</li>
</ol>
<h1 id="优化问题求解"><a href="#优化问题求解" class="headerlink" title="优化问题求解"></a>优化问题求解</h1><p>相关理论参照<a href="https://lyb92nlf.github.io/2021/11/18/SVM数学知识补充/">SVM数学知识补充</a></p>
<h2 id="转换为原问题"><a href="#转换为原问题" class="headerlink" title="转换为原问题"></a>转换为原问题</h2><p>最小值目标函数不变</p>
<p>限制条件</p>
<script type="math/tex; mode=display">
s.t. 1-y_i(w^Tx_i+b)\leq0,i=1,2,\dots,N</script><h2 id="转换为对偶问题"><a href="#转换为对偶问题" class="headerlink" title="转换为对偶问题"></a>转换为对偶问题</h2><script type="math/tex; mode=display">
L(w,b,\alpha)=\frac{1}{2}||w||^2+\sum_{i=1}^N\alpha_i(1-y_i(w^Tx_i+b))</script><script type="math/tex; mode=display">
\frac{\partial L}{\partial w}=w-\sum_{i=1}^N\alpha_iy_ix_i=0\;\;\;\;\frac{\partial L}{\partial b}=\sum_{i=1}^N\alpha_iy_i=0</script><script type="math/tex; mode=display">
\begin{align}
\theta(\alpha)&=\underset{所有w,b}{\inf}\lbrace L(w,b,\alpha)\rbrace \nonumber\\
&=\sum_{i=1}^N\alpha_i-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jx_i^Tx_j\nonumber\\
& s.t. \sum_{i=1}^N\alpha_iy_i=0,\alpha_i\geq0\nonumber\\
&\alpha_i(1-y_i(w^Tx_i+b))=0,y_i(w^Tx_i+b)-1\geq0\nonumber
\end{align}</script><h2 id="求解α"><a href="#求解α" class="headerlink" title="求解α"></a>求解α</h2><p>SMO算法不断执行以下两个步骤知道收敛：</p>
<ul>
<li>选取一对需更新的变量$ \alpha_i,\alpha_j $</li>
<li>固定这两个参数以外的参数，根据上述公式获得更新后的变量</li>
</ul>
<hr>
<hr>
<h1 id="非线性模型"><a href="#非线性模型" class="headerlink" title="非线性模型"></a>非线性模型</h1><h2 id="软间隔"><a href="#软间隔" class="headerlink" title="软间隔"></a>软间隔</h2><p>前面线性模型中，我们假设存在一个超平面能够将不同类别的样本完全划分开。然而，在现实任务中往往很难做到。所以，缓解该问题的一个方法就是允许支持向量在一些样本中出错。</p>
<p><img src="https://raw.githubusercontent.com/lyb92nlf/pictures/master/img/202111211305321.jpg" alt="软间隔" style="zoom:50%;" /></p>
<p>引入松弛变量$ \xi_i $，控制某些样本满足限制条件</p>
<p>引入C（事先设定的参数），限制每个$ \xi _i $，不能让它特别特别大。同时，使目标函数具有最小值</p>
<script type="math/tex; mode=display">
\begin{align}
&最小值：\frac{1}{2}||w||^2+\overbrace{C\sum_{i=1}^N\xi_i}^\text{正则项}\nonumber\\
&s.t. y_i(w^Tx_i+b)\geq1-\xi_i,\xi_i\geq0\nonumber
\end{align}</script><h2 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h2><p>将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分</p>
<script type="math/tex; mode=display">
\underbrace{x}_\text{低维}\longrightarrow\underbrace{\psi(x)}_\text{高维}</script><blockquote>
<p>异或问题（二维空间内线性不可分）：</p>
<script type="math/tex; mode=display">
\begin{align}
&X_1={0\brack 0}\in C_1\;\;\;X_2={1\brack 1}\in C_1\nonumber\\
&X_3={0\brack 1}\in C_2\;\;\;X_4={1\brack 0}\in C_2\nonumber
\end{align}</script><p>定义ψ(x)=[a*a;b*b;a;b;a*b]，使得X为五维向量</p>
<script type="math/tex; mode=display">
\begin{align}
\psi(X_1)=\left[ \begin{matrix}0\\0\\0\\0\\0 \end{matrix}\right]\;\;\;\psi(X_2)=\left[ \begin{matrix}1\\1\\1\\1\\1 \end{matrix}\right]\nonumber\\
\psi(X_3)=\left[ \begin{matrix}1\\0\\1\\0\\0 \end{matrix}\right]\;\;\;\psi(X_4)=\left[ \begin{matrix}0\\1\\0\\1\\0 \end{matrix}\right]\nonumber
\end{align}</script><p>此时存在超平面</p>
<script type="math/tex; mode=display">
w^T\xi (X_i)+b=0,w=\left[ \begin{matrix}-1\\-1\\-1\\-1\\6 \end{matrix}\right]\;\;\;b=1</script><p>使得不同类别的样本线性可分</p>
</blockquote>
<p>那么优化问题变成了</p>
<script type="math/tex; mode=display">
\begin{align}
&最小值：\frac{1}{2}||w||^2+\overbrace{C\sum_{i=1}^N\xi_i}^\text{正则项}\nonumber\\
&s.t. y_i(w^T\psi(x_i)+b)\geq1-\xi_i,\xi_i\geq0\nonumber
\end{align}\tag{优化式}</script><ul>
<li><p>我们可以不知道无限维映射$ \psi(x) $的显示表达，我们只要知道一个<strong>核函数</strong></p>
<script type="math/tex; mode=display">
K(x_1,x_2)=\psi(x_1)^T\psi(x_2)</script><p>则优化式仍然可解</p>
<blockquote>
<p>核函数成立的充要条件：</p>
<script type="math/tex; mode=display">
\begin{align}
&1.\;\;\;K(x_1,x_2)=K(x_2,x_1)\nonumber\\
&2.\;\;\;\forall \underbrace{C_i}_\text{常数},x_i,\sum_{i=1}^N\sum_{j=1}^NC_iC_jK(x_ix_j)
\geq0\nonumber
\end{align}</script></blockquote>
</li>
</ul>
<h2 id="求解"><a href="#求解" class="headerlink" title="求解"></a>求解</h2><p><img src="https://raw.githubusercontent.com/lyb92nlf/pictures/master/img/202111211446322.jpg" style="zoom:50%;" /><img src="https://raw.githubusercontent.com/lyb92nlf/pictures/master/img/202111211446467.jpg" style="zoom:50%;" /></p>
<hr>
<hr>
<h1 id="SVM算法"><a href="#SVM算法" class="headerlink" title="SVM算法"></a>SVM算法</h1><ol>
<li><p>训练流程</p>
<p>输入训练样本，求优化问题。</p>
<p>最大化θ，根据SMO算法求解α</p>
<p>求解b（取一个或所有α∈(0,c)后取平均）</p>
</li>
<li><p>测试流程</p>
<p>输入测试样本</p>
<script type="math/tex; mode=display">
\begin{cases}
\sum_{i=1}^N\alpha_iy_iK(x_i,x)+b\geq0,y=+1\nonumber\\
\sum_{i=1}^N\alpha_iy_iK(x_i,x)+b < 0,y=-1\nonumber
\end{cases}</script></li>
</ol>
]]></content>
      <categories>
        <category>笔记</category>
        <category>西瓜书</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>西瓜书</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch01-图像数据处理操作</title>
    <url>/2022/01/15/pytorch01-%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8A%A0%E8%BD%BD%E5%92%8C%E5%8F%AF%E8%A7%86%E5%8C%96%20-%20%E5%89%AF%E6%9C%AC/</url>
    <content><![CDATA[<p>库学习两大利器</p>
<ul>
<li><p><code>dir()</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">dir</span>(torch))</span><br><span class="line"><span class="comment"># [&#x27;AVG&#x27;, &#x27;AggregationType&#x27;, &#x27;AliasDb&#x27;, &#x27;AnyType&#x27;, &#x27;Argument&#x27;, &#x27;ArgumentSpec&#x27;, &#x27;BFloat16Storage&#x27;, &#x27;BFloat16Tensor&#x27;, &#x27;BenchmarkConfig&#x27;, &#x27;BenchmarkExecutionStats&#x27;, &#x27;Block&#x27;, &#x27;BoolStorage&#x27;, &#x27;BoolTensor&#x27;, &#x27;BoolType&#x27;, &#x27;BufferDict&#x27;, &#x27;ByteStorage&#x27;, &#x27;ByteTensor&#x27;, &#x27;CONV_BN_FUSION&#x27;, ......, &#x27;trunc_&#x27;, &#x27;typename&#x27;, &#x27;types&#x27;, &#x27;uint8&#x27;, &#x27;unbind&#x27;, &#x27;unify_type_list&#x27;, &#x27;unique&#x27;, &#x27;unique_consecutive&#x27;, &#x27;unsafe_chunk&#x27;, &#x27;unsafe_split&#x27;, &#x27;unsafe_split_with_sizes&#x27;, &#x27;unsqueeze&#x27;, &#x27;use_deterministic_algorithms&#x27;, &#x27;utils&#x27;, &#x27;vander&#x27;, &#x27;var&#x27;, &#x27;var_mean&#x27;, &#x27;vdot&#x27;, &#x27;version&#x27;, &#x27;view_as_complex&#x27;, &#x27;view_as_real&#x27;, &#x27;vitals_enabled&#x27;, &#x27;vsplit&#x27;, &#x27;vstack&#x27;, &#x27;wait&#x27;, &#x27;warnings&#x27;, &#x27;where&#x27;, &#x27;with_load_library_flags&#x27;, &#x27;xlogy&#x27;, &#x27;xlogy_&#x27;, &#x27;zero_&#x27;, &#x27;zeros&#x27;, &#x27;zeros_like&#x27;]</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><code>help()</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">help</span>(torch.cuda.is_available)</span><br><span class="line"><span class="comment"># Help on function is_available in module torch.cuda:</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># is_available() -&gt; bool</span></span><br><span class="line"><span class="comment">#     Returns a bool indicating if CUDA is currently available.</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="数据集类-Dataset"><a href="#数据集类-Dataset" class="headerlink" title="数据集类 - Dataset"></a>数据集类 - Dataset</h2><p>抽象类，继承类需要重写<code>__getitem__</code>方法，<code>__len__</code>方法可选</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="comment"># 假设数据集存储在根目录上的data文件夹内，样本的标签就是文件名</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyData</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,data_dir</span>):</span></span><br><span class="line">            <span class="comment"># 初始化数据成员</span></span><br><span class="line">            self.data_dir = data.<span class="built_in">dir</span></span><br><span class="line">            self.img_name_list = os.listdir(data.<span class="built_in">dir</span>) </span><br><span class="line">            </span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self,index</span>):</span></span><br><span class="line">            sample_path = os.path.join(self.data_dir,self.img_name_list[index])</span><br><span class="line">            sample_label = self.img_name_list[index].split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">            sample = Image.<span class="built_in">open</span>(sample_path)</span><br><span class="line">            <span class="keyword">return</span> sample,sample_label</span><br><span class="line">         </span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">len</span>(self.img_name_list)</span><br></pre></td></tr></table></figure>
<h2 id="transforms"><a href="#transforms" class="headerlink" title="transforms"></a>transforms</h2><p>包含对图像进行裁剪、标准化等处理的类</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="comment"># cuda11版本如果安装了cuda10版本会出现userwarming</span></span><br><span class="line"><span class="comment"># 解决方案：</span></span><br><span class="line"><span class="comment"># 	1.pip uninstall torch 	2.pip uninstall torchvision</span></span><br><span class="line"><span class="comment"># 	3.到pytorch官网重新安装对应cuda版本的torch和torchvision</span></span><br></pre></td></tr></table></figure>
<ol>
<li><p>ToTensor类 - 将图像转化为tensor类型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 输入图像类型：PIL/narray</span></span><br><span class="line"></span><br><span class="line">tensor_trans = transforms.ToTensor()</span><br><span class="line"><span class="comment"># 等价于调用__call__(pic)方法</span></span><br><span class="line">tensor_img_narray = tensor_trans(img_cv2)</span><br><span class="line">tensor_img_PIL = tensor_trans(img_PIL)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出图像类型：tensor</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Normalize类 - 对所有通道的像素点进行标准化处理</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 输入图像类型：tensor（不支持PIL图像）</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tensor_img[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 该类初始化时需要设定不同通道的均值和方差</span></span><br><span class="line"><span class="comment"># output[channel] = (input[channel] - mean[channel]) / std[channel]</span></span><br><span class="line">trans_norm = transforms.Normalize([channel1_mean,channel2_mean,channel3_mean],[channel1_std,channel2_std,channel3_std])</span><br><span class="line">img_norm = trans_norm.forward(tensor_img)</span><br><span class="line"><span class="built_in">print</span>(img_norm[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出图像类型：tensor</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Resize类 - 对图像size进行操作</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 输入图像类型：PIL Image or Tensor</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 情况一(h,w)：</span></span><br><span class="line">trans_resize = transforms.Resize((<span class="number">500</span>,<span class="number">500</span>))</span><br><span class="line"><span class="comment"># forward与__call__用法类似，区别是一般__call__包含forward</span></span><br><span class="line">img_resize = trans_resize.forward(tensor_img)</span><br><span class="line"><span class="comment"># img_resize = trans_resize(tensor_img)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 情况二(size):</span></span><br><span class="line"><span class="comment"># if height &gt; width, then image will be rescaled to (size * height / width, size).</span></span><br><span class="line">trans_resize = transforms.Resize(<span class="number">200</span>)</span><br><span class="line">img_resize = trans_resize.forward(tensor_img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出图像类型：取决于输入</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>RandomCrop类 - 对图像进行随机裁剪</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 输入图像类型：PIL Image or Tensor</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 情况一(h,w)：</span></span><br><span class="line">trans_rdC = transforms.RandomCrop((<span class="number">100</span>,<span class="number">200</span>))</span><br><span class="line">img_rdC = trans_rdC.forward(img_PIL)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 情况二(size):</span></span><br><span class="line"><span class="comment"># 裁剪图像的大小为(size,size)</span></span><br><span class="line">trans_rdC = transforms.RandomCrop(<span class="number">100</span>)</span><br><span class="line">img_rdC = trans_rdC(img_PIL)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出图像类型：取决于输入</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Compose类 - 组合transforms操作</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 输入图像类型：取决于第一个transforms操作可以接受的类型</span></span><br><span class="line"></span><br><span class="line">tensor_trans = transforms.ToTensor()</span><br><span class="line">trans_resize = transforms.Resize(<span class="number">200</span>)</span><br><span class="line"><span class="comment"># 列表中前一个输出作为后一个的输入，所以类型要保持一致</span></span><br><span class="line">trans_compose = transforms.Compose([trans_resize,tensor_trans])</span><br><span class="line">img_compose = trans_compose(img_PIL)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出图像类型：取决于最后一个transforms操作输出的类型</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>更多的类可以查看源码学习！！！</p>
<h2 id="tensorboard"><a href="#tensorboard" class="headerlink" title="tensorboard"></a>tensorboard</h2><p>可视化图像和数据，主要介绍两个常用的方法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="comment"># The `SummaryWriter` class provides a high-level API to create an event file in a given directory and add summaries and events to it.</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>) <span class="comment"># 自定义事件文件夹</span></span><br></pre></td></tr></table></figure>
<ol>
<li><p><code>add_scalar(tag,scalar_value,global_step)</code></p>
<blockquote>
<ul>
<li>tag (string): 数据标识（相当于标题，唯一的ID）</li>
<li>scalar_value (float or string/blobname): 存储的值（y轴）</li>
<li>global_step (int): 全局的步数（x轴）</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;y=2x&#x27;</span>,<span class="number">2</span>*i,i)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol>
<li><p><code>add_image(tag,img_tensor,global_step,dataformat)</code></p>
<blockquote>
<ul>
<li>tag (string): 数据标识</li>
<li>img_tensor (torch.Tensor, numpy.array, or string/blobname): 图像数据（tensor或narray）</li>
<li>global_step (int): 全局步数</li>
<li>dataformat(string): 默认tensor类型为’CHW’，对于narray类型需要给定’HWC’</li>
</ul>
</blockquote>
<ol>
<li>图像转化为narray类型的几种方法</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 方法一：cv2</span></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">img_narray = cv2.imread(img_path)</span><br><span class="line"><span class="comment"># 方法二：numpy</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">img_PIL = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line">img_narray = np.array(img_PIL)</span><br></pre></td></tr></table></figure>
<ol>
<li><p>图像转化为tensor类型的方法</p>
<p>详见<code>transforms</code></p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">writer.add_image(<span class="string">&#x27;Image01&#x27;</span>,img_narray,<span class="number">0</span>,dataformat=<span class="string">&#x27;HWC&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>最后一定要关闭！！！</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">writer.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行后在终端输入命令tensorboard --logdir=选取的目录名 --port=指定的端口号</span></span><br><span class="line"><span class="comment"># 端口号默认为6006</span></span><br></pre></td></tr></table></figure>
<h2 id="torchvision中的数据集"><a href="#torchvision中的数据集" class="headerlink" title="torchvision中的数据集"></a>torchvision中的数据集</h2><p><a href="https://pytorch.org/vision/stable/index.html">torchvision说明文档</a>，以CIFAR10为例</p>
<blockquote>
<ul>
<li><strong>root</strong> (<em>string</em>) – Root directory of dataset where directory <code>cifar-10-batches-py</code> exists or will be saved to if download is set to True.</li>
<li><strong>train</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a><em>,</em> <em>optional</em>) – If True, creates dataset from training set, otherwise creates from test set.</li>
<li><strong>transform</strong> (<em>callable**,</em> <em>optional</em>) – A function/transform that takes in an PIL image and returns a transformed version. E.g, <code>transforms.RandomCrop</code></li>
<li><strong>target_transform</strong> (<em>callable**,</em> <em>optional</em>) – A function/transform that takes in the target and transforms it.</li>
<li><strong>download</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a><em>,</em> <em>optional</em>) – If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset_trans = transforms.Compose([</span><br><span class="line">    transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line">train_sets = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset/CIFAR10&#x27;</span>,<span class="literal">True</span>,dataset_trans,download=<span class="literal">True</span>)</span><br><span class="line">test_sets = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset/CIFAR10&#x27;</span>,<span class="literal">False</span>,dataset_trans,download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># img_tensor,target = train_sets[0]</span></span><br><span class="line"><span class="comment"># print(img_tensor,train_sets.classes[target])</span></span><br><span class="line"><span class="comment"># print(len(train_sets))</span></span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;./logs/dataset&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    writer.add_image(<span class="string">&#x27;train_sets&#x27;</span>,train_sets[i][<span class="number">0</span>],i)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    writer.add_image(<span class="string">&#x27;test_sets&#x27;</span>,test_sets[i][<span class="number">0</span>],i)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<h2 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a>DataLoader</h2><p>每次从Dataset中选取batch_size个样本</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line">test_sets = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset/CIFAR10&#x27;</span>,<span class="literal">False</span>,transform=transforms.ToTensor(),download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(test_sets)) <span class="comment"># 10000</span></span><br><span class="line"><span class="comment"># shuffle：是否随机选取；drop_last：图像数量不足batch_size，是否丢弃</span></span><br><span class="line">dl = DataLoader(test_sets,batch_size=<span class="number">64</span>,shuffle=<span class="literal">True</span>,drop_last=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(dl)) <span class="comment"># drop_last=True:156 ; drop_last=False:157</span></span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./logs/dataloader&#x27;</span>)</span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> dl:</span><br><span class="line">    imgs,targets = item</span><br><span class="line">    <span class="built_in">print</span>(imgs.shape,targets) <span class="comment"># torch.Size([6, 3, 32, 32]) tensor([8, 4, 6, 5, 6, 3])</span></span><br><span class="line">    <span class="comment"># 某个步骤同时显示多个图像</span></span><br><span class="line">    writer.add_images(<span class="string">&#x27;CIFAR10_Test&#x27;</span>,imgs,step)</span><br><span class="line">    step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Python</category>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
        <tag>Dataset</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch02-神经网络的“组件”</title>
    <url>/2022/01/18/pytorch02-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E2%80%9C%E7%BB%84%E4%BB%B6%E2%80%9D/</url>
    <content><![CDATA[<p><a href="https://pytorch.org/docs/stable/nn.html">torch.nn文档在这里！！！</a></p>
<h2 id="Containers"><a href="#Containers" class="headerlink" title="Containers"></a>Containers</h2><h3 id="Moudle"><a href="#Moudle" class="headerlink" title="Moudle"></a>Moudle</h3><p>对于所有神经网络的基础类。自定义模型需要继承这个类，必须重写<code>forward</code>方法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">20</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line">        self.avgPool1 = nn.AvgPool2d(<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.relu1 = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.avgPool1(x)</span><br><span class="line">        x = self.relu1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h2><h3 id="Conv2d"><a href="#Conv2d" class="headerlink" title="Conv2d"></a>Conv2d</h3><p><strong>参数列表</strong></p>
<ul>
<li><strong>in_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – 输入图像的通道数</li>
<li><strong>out_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – 经过卷积后输出图像的通道数</li>
<li><strong>kernel_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a>) – 卷积核的尺寸</li>
<li><strong>stride</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a><em>,</em> <em>optional</em>) – 卷积的步长。默认: 1</li>
<li><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>,</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#str"><em>str</em></a><em>,</em> <em>optional</em>) – 对输入图像的边缘进行填充。默认: 0</li>
<li><strong>padding_mode</strong> (<em>string**,</em> <em>optional</em>) – 填充模式。默认：零填充<ul>
<li><code>&#39;zeros&#39;</code> - 零填充</li>
<li><code>&#39;reflect&#39;</code> - 以矩阵边缘为对称轴，将矩阵中的元素对称的填充到最外围</li>
<li><code>&#39;replicate&#39;</code> - 将矩阵的边缘复制并填充到矩阵的外围</li>
<li><code>`&#39;circular&#39;</code> - 循环的进行填充</li>
</ul>
</li>
<li><strong>dilation</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a><em>,</em> <em>optional</em>) – 膨胀系数。默认: 1</li>
<li><strong>groups</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>,</em> <em>optional</em>) – 分组的个数。默认: 1</li>
<li><strong>bias</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a><em>,</em> <em>optional</em>) – 如果<code>True</code>, 为输出添加一个可学习的偏置。默认: <code>True</code></li>
</ul>
<h2 id="池化层-减少参数"><a href="#池化层-减少参数" class="headerlink" title="池化层 - 减少参数"></a>池化层 - 减少参数</h2><h3 id="MaxPool2d"><a href="#MaxPool2d" class="headerlink" title="MaxPool2d"></a>MaxPool2d</h3><p><strong>参数列表</strong></p>
<ul>
<li><strong>kernel_size</strong> – 卷积核（无参）的大小</li>
<li><strong>stride</strong> – 步长。默认值是<code>kernel_size</code></li>
<li><strong>padding</strong> – 零填充各个边缘的围数。</li>
<li><strong>dilation</strong> – 膨胀系数</li>
<li><strong>return_indices</strong> – 暂时用不到</li>
<li><strong>ceil_mode</strong> – 如果<code>True</code>, 即使图像不能填满卷积核也算一次</li>
</ul>
<h3 id="AvgPool2d"><a href="#AvgPool2d" class="headerlink" title="AvgPool2d"></a>AvgPool2d</h3><p><strong>参数列表</strong></p>
<ul>
<li><strong>kernel_size</strong> – 卷积核（无参）的大小</li>
<li><strong>stride</strong> – 步长。默认值是<code>kernel_size</code></li>
<li><strong>padding</strong> – 零填充各个边缘的围数。</li>
<li><strong>ceil_mode</strong> – 如果<code>True</code>, 即使图像不能填满卷积核也算一次</li>
<li><strong>count_include_pad</strong> – 如果<code>True</code>，在计算平均值时包括零填充的边</li>
<li><strong>divisor_override</strong> – 指定除数，否则就是卷积核的尺寸</li>
</ul>
<h2 id="非线性激活-提高泛化能力"><a href="#非线性激活-提高泛化能力" class="headerlink" title="非线性激活 - 提高泛化能力"></a>非线性激活 - 提高泛化能力</h2><h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><p><strong>参数列表</strong></p>
<ul>
<li><strong>inplace</strong> – 是否直接替换处理的值。默认: <code>False</code></li>
</ul>
<h3 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h3><h3 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h3><p><strong>参数列表</strong></p>
<ul>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>)<ul>
<li>当 dim=0 时，是对每一维度相同位置的数值进行softmax运算；</li>
<li>当 dim=1 时，是对某一维度的列进行softmax运算；</li>
<li>当 dim=2 或 -1 时，是对某一维度的行进行softmax运算；</li>
</ul>
</li>
</ul>
<p>​    <img src="https://raw.githubusercontent.com/lyb92nlf/pictures/master/img/202201191155696.png" alt="dim"></p>
<h2 id="标准化层"><a href="#标准化层" class="headerlink" title="标准化层"></a>标准化层</h2><p><a href="https://zhuanlan.zhihu.com/p/75603087">好处</a></p>
<h3 id="BatchNorm2d"><a href="#BatchNorm2d" class="headerlink" title="BatchNorm2d"></a>BatchNorm2d</h3><p><strong>参数列表</strong></p>
<ul>
<li><strong>num_features</strong> – (<em>N</em>,<em>C</em>,<em>H</em>,<em>W</em>)中的通道数</li>
<li><strong>affine</strong> – 如果<code>True</code>，添加一个可学习的参数。默认：<code>True</code></li>
</ul>
<h2 id="线性层"><a href="#线性层" class="headerlink" title="线性层"></a>线性层</h2><h3 id="Linear"><a href="#Linear" class="headerlink" title="Linear"></a>Linear</h3><p><strong>参数列表</strong></p>
<ul>
<li><strong>in_features</strong> – 输入个数</li>
<li><strong>out_features</strong> – 输出个数</li>
<li><strong>bias</strong> – 如果<code>True</code>, 添加一个可学习的偏置。默认: <code>True</code></li>
</ul>
<p>常用函数<code>torch.reshape</code>或<code>torch.flatten</code></p>
<h2 id="Dropout层"><a href="#Dropout层" class="headerlink" title="Dropout层"></a>Dropout层</h2><h3 id="Dropout2d"><a href="#Dropout2d" class="headerlink" title="Dropout2d"></a>Dropout2d</h3><p><strong>参数列表</strong></p>
<ul>
<li><strong>p</strong> (<a href="https://docs.python.org/3/library/functions.html#float"><em>float</em></a><em>,</em> <em>optional</em>) – 整个通道被替换成0的可能性</li>
<li><strong>inplace</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a><em>,</em> <em>optional</em>) – 是否直接替换处理的值。默认: <code>False</code></li>
</ul>
]]></content>
      <categories>
        <category>Python</category>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>样条插值和最小二乘法拟合</title>
    <url>/2021/11/20/%E6%A0%B7%E6%9D%A1%E6%8F%92%E5%80%BC%E5%92%8C%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%8B%9F%E5%90%88/</url>
    <content><![CDATA[<h1 id="最小二乘法曲线拟合"><a href="#最小二乘法曲线拟合" class="headerlink" title="最小二乘法曲线拟合"></a>最小二乘法曲线拟合</h1><h2 id="数学公式"><a href="#数学公式" class="headerlink" title="数学公式"></a>数学公式</h2><p>拟合多项式为</p>
<script type="math/tex; mode=display">
f(x)=a_0+a_1x+a_2x_2+\dots+a_mx_m</script><p>将n个数据带入得到矩阵</p>
<script type="math/tex; mode=display">
X=\left[ \begin{matrix} 1 & x_1 & x_1^2 & \cdots & x_1^m\\ 1 & x_2 & x_2^2 & \cdots & x_2^m \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & x_n & x_n^2 & \cdots & x_n^m \end{matrix} \right]</script><p>组成矩阵方程为</p>
<script type="math/tex; mode=display">
\begin{align}
X^TXa&=\left[ \begin{matrix} n & \sum_{i=1}^nx_i & \sum_{i=1}^nx_i^2 & \cdots & \sum_{i=1}^nx_i^m\\ \sum_{i=1}^nx_i & \sum_{i=1}^nx_i^2 & \sum_{i=1}^nx_i^3 & \cdots & \sum_{i=1}^nx_i^{m+1} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ \sum_{i=1}^nx_i^m & \sum_{i=1}^nx_i^{m+1} & \sum_{i=1}^nx_i^{m+2} & \cdots & \sum_{i=1}^nx_i^{2m} \end{matrix} \right]\left[ \begin{matrix}a_0\\a_1\\a_2\\\vdots\\a_m\end{matrix} \right]\nonumber\\
&=\left[ \begin{matrix}\sum_{i=1}^ny_i\\\sum_{i=1}^ny_ix_i\\\sum_{i=1}^ny_ix_i^2\\\vdots\\\sum_{i=1}^ny_ix_i^m\end{matrix} \right]=X^Ty\nonumber
\end{align}</script><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- codeing = utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time : 2021/11/15 09:45</span></span><br><span class="line"><span class="comment"># @Author : Lyb92nlf</span></span><br><span class="line"><span class="comment"># @File : CSI.py</span></span><br><span class="line"><span class="comment"># @Sofeware : PyCharm</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a0+a1x+a2x2</span></span><br><span class="line">w = np.zeros(<span class="number">3</span>)</span><br><span class="line">points = np.genfromtxt(<span class="string">&quot;data.csv&quot;</span>, delimiter=<span class="string">&quot;,&quot;</span>)</span><br><span class="line"><span class="comment"># 提取points中的两列数据，分别作为x,y</span></span><br><span class="line">x = points[:, <span class="number">0</span>]</span><br><span class="line">y = points[:, <span class="number">1</span>]</span><br><span class="line"><span class="comment"># print(x,y)</span></span><br><span class="line"><span class="comment"># print(max(x),min(x))</span></span><br><span class="line"></span><br><span class="line">X = np.zeros([<span class="built_in">len</span>(x), <span class="built_in">len</span>(w)])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x)):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(w)):</span><br><span class="line">        X[i, j] = x[i] ** j</span><br><span class="line"><span class="comment"># print(X)</span></span><br><span class="line"><span class="comment"># w = (X^T X)^(-1) X^T y</span></span><br><span class="line">XTX = np.matmul(X.T, X)</span><br><span class="line">XTy = np.matmul(X.T, y)</span><br><span class="line">w = np.matmul(np.linalg.inv(XTX), XTy)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># print(XTX)</span></span><br><span class="line"><span class="comment"># print(y.shape)</span></span><br><span class="line"><span class="built_in">print</span>(w)</span><br><span class="line"><span class="comment"># 构造函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Fun</span>(<span class="params">p, x</span>):</span></span><br><span class="line">    a0, a1, a2 = p</span><br><span class="line">    <span class="keyword">return</span> a2 * x ** <span class="number">2</span> + a1 * x + a0</span><br><span class="line"></span><br><span class="line">fun_x = np.arange(<span class="built_in">min</span>(x),<span class="built_in">max</span>(x),<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(x, y, color=<span class="string">&#x27;g&#x27;</span>, linestyle=<span class="string">&#x27;&#x27;</span>, marker=<span class="string">&#x27;.&#x27;</span>, label=<span class="string">&quot;Data points&quot;</span>)</span><br><span class="line">plt.plot(fun_x, Fun(w, fun_x), color=<span class="string">&#x27;r&#x27;</span>, linestyle=<span class="string">&#x27;-&#x27;</span>, marker=<span class="string">&#x27;&#x27;</span>, label=<span class="string">&quot;Fitting curve&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h1 id="三次样条差值"><a href="#三次样条差值" class="headerlink" title="三次样条差值"></a>三次样条差值</h1><h2 id="数学公式-1"><a href="#数学公式-1" class="headerlink" title="数学公式"></a>数学公式</h2><script type="math/tex; mode=display">
h[i]=x[i+1]-x[i]\;\;u[i]=\frac{h[i]}{h[i]+h[i+1]}\;\;\lambda[i]=1-u[i]</script><script type="math/tex; mode=display">
\begin{align}
d[i]&=6f[x[i-1],x[i],x[i+1]]\nonumber\\
&=6\times\frac{f[x[i],f[i+1]]-f[[i-1],f[i]]}{x[i+1]-x[i-1]}\nonumber
\end{align}</script><script type="math/tex; mode=display">
f[x[i],x[i+1]]=\frac{y[i+1]-y[i]}{h[i]}</script><script type="math/tex; mode=display">
\left[ \begin{matrix} 2 & \lambda_1 &  &  &  & \\ u_2 & 2 & \lambda_2 &  & & \\  & u_3 & 2 & \lambda_3 &  &  \\  & & \ddots & \ddots & \ddots & \\ &  &  & u_{n-2} & 2 & \lambda_{n-1}\\ &  &  &  & u_{n-1} & 2 \end{matrix} \right]
\left[ \begin{matrix}M_1\\M_2\\M_3\\\vdots\\M_{n-1}\end{matrix} \right]=\left[ \begin{matrix}d_1\\d_2\\d_3\\\vdots\\d_{n-1}\end{matrix} \right]</script><script type="math/tex; mode=display">
S(x)=\frac{M_j(x_{j+1}-x)^3+M_{j+1}(x-x_j)^3}{6h_j}+(y_{j+1}-\frac{M_{j+1}h_j^2}{6})\frac{x-x_j}{h_j}+(y_j-\frac{M_jh_j^2}{6})\frac{x_{j+1}-x}{h_j}</script><h2 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- codeing = utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time : 2021/11/15 12:11</span></span><br><span class="line"><span class="comment"># @Author : Lyb92nlf</span></span><br><span class="line"><span class="comment"># @File : CSI.py</span></span><br><span class="line"><span class="comment"># @Sofeware : PyCharm</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sympy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">points = np.genfromtxt(<span class="string">&quot;data.csv&quot;</span>, delimiter=<span class="string">&quot;,&quot;</span>)</span><br><span class="line"><span class="comment"># 提取points中的两列数据，分别作为x,y</span></span><br><span class="line"><span class="comment"># 1:6</span></span><br><span class="line">x = points[<span class="number">1</span>:<span class="number">6</span>, <span class="number">0</span>]</span><br><span class="line">y = points[<span class="number">1</span>:<span class="number">6</span>, <span class="number">1</span>]</span><br><span class="line"><span class="comment"># print(x, y)</span></span><br><span class="line"></span><br><span class="line">n = <span class="built_in">len</span>(x) - <span class="number">1</span></span><br><span class="line">h = np.zeros(n)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    h[i] = x[i + <span class="number">1</span>] - x[i]</span><br><span class="line"><span class="comment"># print(h)</span></span><br><span class="line"></span><br><span class="line">u = np.zeros(n - <span class="number">1</span>)</span><br><span class="line">l = np.zeros(n - <span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n - <span class="number">1</span>):</span><br><span class="line">    u[i] = h[i] / (h[i] + h[i + <span class="number">1</span>])</span><br><span class="line">    l[i] = <span class="number">1</span> - u[i]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;u=&quot;</span>, u, <span class="string">&quot; &quot;</span>, <span class="string">&quot;l=&quot;</span>, l)</span><br><span class="line"></span><br><span class="line">d = np.zeros(n - <span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n - <span class="number">1</span>):</span><br><span class="line">    d[i] = <span class="number">6</span> / (h[i] + h[i + <span class="number">1</span>]) * ((y[i + <span class="number">2</span>] - y[i + <span class="number">1</span>]) / h[i + <span class="number">1</span>] - (y[i + <span class="number">1</span>] - y[i]) / h[i])</span><br><span class="line"><span class="comment"># print(d)</span></span><br><span class="line"></span><br><span class="line">m = np.zeros(n + <span class="number">1</span>)</span><br><span class="line">A = np.zeros([n - <span class="number">1</span>, n - <span class="number">1</span>])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n - <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n - <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">if</span> i == j:</span><br><span class="line">            A[i, j] = <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> i + <span class="number">1</span> == j:</span><br><span class="line">            A[i, j] = l[i]</span><br><span class="line">        <span class="keyword">if</span> j + <span class="number">1</span> == i:</span><br><span class="line">            A[i, j] = u[i]</span><br><span class="line"><span class="comment"># print(A)</span></span><br><span class="line"></span><br><span class="line">m[<span class="number">1</span>:<span class="number">4</span>] = np.matmul(np.linalg.inv(A), d)</span><br><span class="line"><span class="comment"># print(m)</span></span><br><span class="line"></span><br><span class="line">S = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示分段函数</span></span><br><span class="line">val_x = symbols(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line"><span class="comment"># (x-y)**3 = x**3 - 3x**2 y + 3xy**2 - y**3</span></span><br><span class="line"><span class="comment"># (x[i+1] - val_x) ** 3</span></span><br><span class="line"><span class="comment"># (val_x - x[i]) ** 3</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, n):</span><br><span class="line">    s = m[i] * ((x[i + <span class="number">1</span>]) ** <span class="number">3</span> - <span class="number">3</span> * (x[i + <span class="number">1</span>]) ** <span class="number">2</span> * val_x + <span class="number">3</span> * x[i + <span class="number">1</span>] * val_x ** <span class="number">2</span> - val_x ** <span class="number">3</span>) / (<span class="number">6</span> * h[i]) + \</span><br><span class="line">        m[i + <span class="number">1</span>] * (val_x ** <span class="number">3</span> - <span class="number">3</span> * val_x ** <span class="number">2</span> * x[i] + <span class="number">3</span> * val_x * (x[i]) ** <span class="number">2</span> - x[i] ** <span class="number">3</span>) / (<span class="number">6</span> * h[i]) + (</span><br><span class="line">                    y[i + <span class="number">1</span>] - (h[i] ** <span class="number">2</span>) * m[i + <span class="number">1</span>] / <span class="number">6</span>) * (val_x - x[i]) / h[i] + (y[i] - (h[i] ** <span class="number">2</span>) * m[i] / <span class="number">6</span>) * (</span><br><span class="line">                    x[i + <span class="number">1</span>] - val_x) / h[i]</span><br><span class="line">    S.append(s)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(S)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算分段函数值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">s_value</span>(<span class="params">i, fun_x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> m[i] * ((x[i + <span class="number">1</span>] - fun_x) ** <span class="number">3</span>) / (<span class="number">6</span> * h[i]) + m[i + <span class="number">1</span>] * ((fun_x - x[i]) ** <span class="number">3</span>) / (<span class="number">6</span> * h[i]) + (</span><br><span class="line">                y[i + <span class="number">1</span>] - (h[i] ** <span class="number">2</span>) * m[i + <span class="number">1</span>] / <span class="number">6</span>) * (fun_x - x[i]) / h[i] + (y[i] - (h[i] ** <span class="number">2</span>) * m[i] / <span class="number">6</span>) * (</span><br><span class="line">                       x[i + <span class="number">1</span>] - fun_x) / h[i]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.plot(x, y, color=<span class="string">&#x27;g&#x27;</span>, linestyle=<span class="string">&#x27;&#x27;</span>, marker=<span class="string">&#x27;.&#x27;</span>, label=<span class="string">&quot;Data points&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    fun_x = np.arange(x[i], x[i + <span class="number">1</span>], <span class="number">0.005</span>)</span><br><span class="line">    plt.plot(fun_x, s_value(i, fun_x), color=<span class="string">&#x27;r&#x27;</span>, linestyle=<span class="string">&#x27;-&#x27;</span>, marker=<span class="string">&#x27;&#x27;</span>, label=<span class="string">&quot;Fitting curve&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>))</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h1 id="数据格式"><a href="#数据格式" class="headerlink" title="数据格式"></a>数据格式</h1><p>data.csv</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0,0</span><br><span class="line">5,1.27</span><br><span class="line">10,2.16</span><br><span class="line">15,2.86</span><br><span class="line">20,3.44</span><br><span class="line">25,3.87</span><br><span class="line">30,4.15</span><br><span class="line">35,4.37</span><br><span class="line">40,4.51</span><br><span class="line">45,4.58</span><br><span class="line">50,4.62</span><br><span class="line">55,4.64</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>笔记</category>
        <category>数学知识</category>
      </categories>
      <tags>
        <tag>数值分析</tag>
        <tag>样条插值</tag>
        <tag>最小二乘法曲线拟合</tag>
        <tag>python</tag>
      </tags>
  </entry>
</search>
